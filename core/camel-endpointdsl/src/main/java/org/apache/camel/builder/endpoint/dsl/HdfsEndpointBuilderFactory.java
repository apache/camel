/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.camel.builder.endpoint.dsl;

import java.util.Map;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;
import javax.annotation.Generated;
import org.apache.camel.ExchangePattern;
import org.apache.camel.LoggingLevel;
import org.apache.camel.builder.EndpointConsumerBuilder;
import org.apache.camel.builder.EndpointProducerBuilder;
import org.apache.camel.builder.endpoint.AbstractEndpointBuilder;
import org.apache.camel.spi.ExceptionHandler;
import org.apache.camel.spi.PollingConsumerPollStrategy;
import org.apache.camel.spi.ScheduledPollConsumerScheduler;

/**
 * For reading/writing from/to an HDFS filesystem using Hadoop 2.x.
 * 
 * Generated by camel-package-maven-plugin - do not edit this file!
 */
@Generated("org.apache.camel.maven.packaging.EndpointDslMojo")
public interface HdfsEndpointBuilderFactory {


    /**
     * Builder for endpoint consumers for the HDFS2 component.
     */
    public interface HdfsEndpointConsumerBuilder
            extends
                EndpointConsumerBuilder {
        default AdvancedHdfsEndpointConsumerBuilder advanced() {
            return (AdvancedHdfsEndpointConsumerBuilder) this;
        }
        /**
         * HDFS host to use.
         * The option is a <code>java.lang.String</code> type.
         * @group common
         */
        default HdfsEndpointConsumerBuilder hostName(String hostName) {
            setProperty("hostName", hostName);
            return this;
        }
        /**
         * HDFS port to use.
         * The option is a <code>int</code> type.
         * @group common
         */
        default HdfsEndpointConsumerBuilder port(int port) {
            setProperty("port", port);
            return this;
        }
        /**
         * HDFS port to use.
         * The option will be converted to a <code>int</code> type.
         * @group common
         */
        default HdfsEndpointConsumerBuilder port(String port) {
            setProperty("port", port);
            return this;
        }
        /**
         * The directory path to use.
         * The option is a <code>java.lang.String</code> type.
         * @group common
         */
        default HdfsEndpointConsumerBuilder path(String path) {
            setProperty("path", path);
            return this;
        }
        /**
         * Whether to connect to the HDFS file system on starting the
         * producer/consumer. If false then the connection is created on-demand.
         * Notice that HDFS may take up till 15 minutes to establish a
         * connection, as it has hardcoded 45 x 20 sec redelivery. By setting
         * this option to false allows your application to startup, and not
         * block for up till 15 minutes.
         * The option is a <code>boolean</code> type.
         * @group common
         */
        default HdfsEndpointConsumerBuilder connectOnStartup(
                boolean connectOnStartup) {
            setProperty("connectOnStartup", connectOnStartup);
            return this;
        }
        /**
         * Whether to connect to the HDFS file system on starting the
         * producer/consumer. If false then the connection is created on-demand.
         * Notice that HDFS may take up till 15 minutes to establish a
         * connection, as it has hardcoded 45 x 20 sec redelivery. By setting
         * this option to false allows your application to startup, and not
         * block for up till 15 minutes.
         * The option will be converted to a <code>boolean</code> type.
         * @group common
         */
        default HdfsEndpointConsumerBuilder connectOnStartup(
                String connectOnStartup) {
            setProperty("connectOnStartup", connectOnStartup);
            return this;
        }
        /**
         * Set to LOCAL to not use HDFS but local java.io.File instead.
         * The option is a
         * <code>org.apache.camel.component.hdfs2.HdfsFileSystemType</code>
         * type.
         * @group common
         */
        default HdfsEndpointConsumerBuilder fileSystemType(
                HdfsFileSystemType fileSystemType) {
            setProperty("fileSystemType", fileSystemType);
            return this;
        }
        /**
         * Set to LOCAL to not use HDFS but local java.io.File instead.
         * The option will be converted to a
         * <code>org.apache.camel.component.hdfs2.HdfsFileSystemType</code>
         * type.
         * @group common
         */
        default HdfsEndpointConsumerBuilder fileSystemType(String fileSystemType) {
            setProperty("fileSystemType", fileSystemType);
            return this;
        }
        /**
         * The file type to use. For more details see Hadoop HDFS documentation
         * about the various files types.
         * The option is a
         * <code>org.apache.camel.component.hdfs2.HdfsFileType</code> type.
         * @group common
         */
        default HdfsEndpointConsumerBuilder fileType(HdfsFileType fileType) {
            setProperty("fileType", fileType);
            return this;
        }
        /**
         * The file type to use. For more details see Hadoop HDFS documentation
         * about the various files types.
         * The option will be converted to a
         * <code>org.apache.camel.component.hdfs2.HdfsFileType</code> type.
         * @group common
         */
        default HdfsEndpointConsumerBuilder fileType(String fileType) {
            setProperty("fileType", fileType);
            return this;
        }
        /**
         * The type for the key in case of sequence or map files.
         * The option is a
         * <code>org.apache.camel.component.hdfs2.WritableType</code> type.
         * @group common
         */
        default HdfsEndpointConsumerBuilder keyType(WritableType keyType) {
            setProperty("keyType", keyType);
            return this;
        }
        /**
         * The type for the key in case of sequence or map files.
         * The option will be converted to a
         * <code>org.apache.camel.component.hdfs2.WritableType</code> type.
         * @group common
         */
        default HdfsEndpointConsumerBuilder keyType(String keyType) {
            setProperty("keyType", keyType);
            return this;
        }
        /**
         * The file owner must match this owner for the consumer to pickup the
         * file. Otherwise the file is skipped.
         * The option is a <code>java.lang.String</code> type.
         * @group common
         */
        default HdfsEndpointConsumerBuilder owner(String owner) {
            setProperty("owner", owner);
            return this;
        }
        /**
         * The type for the key in case of sequence or map files.
         * The option is a
         * <code>org.apache.camel.component.hdfs2.WritableType</code> type.
         * @group common
         */
        default HdfsEndpointConsumerBuilder valueType(WritableType valueType) {
            setProperty("valueType", valueType);
            return this;
        }
        /**
         * The type for the key in case of sequence or map files.
         * The option will be converted to a
         * <code>org.apache.camel.component.hdfs2.WritableType</code> type.
         * @group common
         */
        default HdfsEndpointConsumerBuilder valueType(String valueType) {
            setProperty("valueType", valueType);
            return this;
        }
        /**
         * Allows for bridging the consumer to the Camel routing Error Handler,
         * which mean any exceptions occurred while the consumer is trying to
         * pickup incoming messages, or the likes, will now be processed as a
         * message and handled by the routing Error Handler. By default the
         * consumer will use the org.apache.camel.spi.ExceptionHandler to deal
         * with exceptions, that will be logged at WARN or ERROR level and
         * ignored.
         * The option is a <code>boolean</code> type.
         * @group consumer
         */
        default HdfsEndpointConsumerBuilder bridgeErrorHandler(
                boolean bridgeErrorHandler) {
            setProperty("bridgeErrorHandler", bridgeErrorHandler);
            return this;
        }
        /**
         * Allows for bridging the consumer to the Camel routing Error Handler,
         * which mean any exceptions occurred while the consumer is trying to
         * pickup incoming messages, or the likes, will now be processed as a
         * message and handled by the routing Error Handler. By default the
         * consumer will use the org.apache.camel.spi.ExceptionHandler to deal
         * with exceptions, that will be logged at WARN or ERROR level and
         * ignored.
         * The option will be converted to a <code>boolean</code> type.
         * @group consumer
         */
        default HdfsEndpointConsumerBuilder bridgeErrorHandler(
                String bridgeErrorHandler) {
            setProperty("bridgeErrorHandler", bridgeErrorHandler);
            return this;
        }
        /**
         * The pattern used for scanning the directory.
         * The option is a <code>java.lang.String</code> type.
         * @group consumer
         */
        default HdfsEndpointConsumerBuilder pattern(String pattern) {
            setProperty("pattern", pattern);
            return this;
        }
        /**
         * If the polling consumer did not poll any files, you can enable this
         * option to send an empty message (no body) instead.
         * The option is a <code>boolean</code> type.
         * @group consumer
         */
        default HdfsEndpointConsumerBuilder sendEmptyMessageWhenIdle(
                boolean sendEmptyMessageWhenIdle) {
            setProperty("sendEmptyMessageWhenIdle", sendEmptyMessageWhenIdle);
            return this;
        }
        /**
         * If the polling consumer did not poll any files, you can enable this
         * option to send an empty message (no body) instead.
         * The option will be converted to a <code>boolean</code> type.
         * @group consumer
         */
        default HdfsEndpointConsumerBuilder sendEmptyMessageWhenIdle(
                String sendEmptyMessageWhenIdle) {
            setProperty("sendEmptyMessageWhenIdle", sendEmptyMessageWhenIdle);
            return this;
        }
        /**
         * The number of subsequent error polls (failed due some error) that
         * should happen before the backoffMultipler should kick-in.
         * The option is a <code>int</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder backoffErrorThreshold(
                int backoffErrorThreshold) {
            setProperty("backoffErrorThreshold", backoffErrorThreshold);
            return this;
        }
        /**
         * The number of subsequent error polls (failed due some error) that
         * should happen before the backoffMultipler should kick-in.
         * The option will be converted to a <code>int</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder backoffErrorThreshold(
                String backoffErrorThreshold) {
            setProperty("backoffErrorThreshold", backoffErrorThreshold);
            return this;
        }
        /**
         * The number of subsequent idle polls that should happen before the
         * backoffMultipler should kick-in.
         * The option is a <code>int</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder backoffIdleThreshold(
                int backoffIdleThreshold) {
            setProperty("backoffIdleThreshold", backoffIdleThreshold);
            return this;
        }
        /**
         * The number of subsequent idle polls that should happen before the
         * backoffMultipler should kick-in.
         * The option will be converted to a <code>int</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder backoffIdleThreshold(
                String backoffIdleThreshold) {
            setProperty("backoffIdleThreshold", backoffIdleThreshold);
            return this;
        }
        /**
         * To let the scheduled polling consumer backoff if there has been a
         * number of subsequent idles/errors in a row. The multiplier is then
         * the number of polls that will be skipped before the next actual
         * attempt is happening again. When this option is in use then
         * backoffIdleThreshold and/or backoffErrorThreshold must also be
         * configured.
         * The option is a <code>int</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder backoffMultiplier(
                int backoffMultiplier) {
            setProperty("backoffMultiplier", backoffMultiplier);
            return this;
        }
        /**
         * To let the scheduled polling consumer backoff if there has been a
         * number of subsequent idles/errors in a row. The multiplier is then
         * the number of polls that will be skipped before the next actual
         * attempt is happening again. When this option is in use then
         * backoffIdleThreshold and/or backoffErrorThreshold must also be
         * configured.
         * The option will be converted to a <code>int</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder backoffMultiplier(
                String backoffMultiplier) {
            setProperty("backoffMultiplier", backoffMultiplier);
            return this;
        }
        /**
         * Milliseconds before the next poll. You can also specify time values
         * using units, such as 60s (60 seconds), 5m30s (5 minutes and 30
         * seconds), and 1h (1 hour).
         * The option is a <code>long</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder delay(long delay) {
            setProperty("delay", delay);
            return this;
        }
        /**
         * Milliseconds before the next poll. You can also specify time values
         * using units, such as 60s (60 seconds), 5m30s (5 minutes and 30
         * seconds), and 1h (1 hour).
         * The option will be converted to a <code>long</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder delay(String delay) {
            setProperty("delay", delay);
            return this;
        }
        /**
         * If greedy is enabled, then the ScheduledPollConsumer will run
         * immediately again, if the previous run polled 1 or more messages.
         * The option is a <code>boolean</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder greedy(boolean greedy) {
            setProperty("greedy", greedy);
            return this;
        }
        /**
         * If greedy is enabled, then the ScheduledPollConsumer will run
         * immediately again, if the previous run polled 1 or more messages.
         * The option will be converted to a <code>boolean</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder greedy(String greedy) {
            setProperty("greedy", greedy);
            return this;
        }
        /**
         * Milliseconds before the first poll starts. You can also specify time
         * values using units, such as 60s (60 seconds), 5m30s (5 minutes and 30
         * seconds), and 1h (1 hour).
         * The option is a <code>long</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder initialDelay(long initialDelay) {
            setProperty("initialDelay", initialDelay);
            return this;
        }
        /**
         * Milliseconds before the first poll starts. You can also specify time
         * values using units, such as 60s (60 seconds), 5m30s (5 minutes and 30
         * seconds), and 1h (1 hour).
         * The option will be converted to a <code>long</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder initialDelay(String initialDelay) {
            setProperty("initialDelay", initialDelay);
            return this;
        }
        /**
         * The consumer logs a start/complete log line when it polls. This
         * option allows you to configure the logging level for that.
         * The option is a <code>org.apache.camel.LoggingLevel</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder runLoggingLevel(
                LoggingLevel runLoggingLevel) {
            setProperty("runLoggingLevel", runLoggingLevel);
            return this;
        }
        /**
         * The consumer logs a start/complete log line when it polls. This
         * option allows you to configure the logging level for that.
         * The option will be converted to a
         * <code>org.apache.camel.LoggingLevel</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder runLoggingLevel(
                String runLoggingLevel) {
            setProperty("runLoggingLevel", runLoggingLevel);
            return this;
        }
        /**
         * Allows for configuring a custom/shared thread pool to use for the
         * consumer. By default each consumer has its own single threaded thread
         * pool.
         * The option is a
         * <code>java.util.concurrent.ScheduledExecutorService</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder scheduledExecutorService(
                ScheduledExecutorService scheduledExecutorService) {
            setProperty("scheduledExecutorService", scheduledExecutorService);
            return this;
        }
        /**
         * Allows for configuring a custom/shared thread pool to use for the
         * consumer. By default each consumer has its own single threaded thread
         * pool.
         * The option will be converted to a
         * <code>java.util.concurrent.ScheduledExecutorService</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder scheduledExecutorService(
                String scheduledExecutorService) {
            setProperty("scheduledExecutorService", scheduledExecutorService);
            return this;
        }
        /**
         * To use a cron scheduler from either camel-spring or camel-quartz2
         * component.
         * The option is a
         * <code>org.apache.camel.spi.ScheduledPollConsumerScheduler</code>
         * type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder scheduler(
                ScheduledPollConsumerScheduler scheduler) {
            setProperty("scheduler", scheduler);
            return this;
        }
        /**
         * To use a cron scheduler from either camel-spring or camel-quartz2
         * component.
         * The option will be converted to a
         * <code>org.apache.camel.spi.ScheduledPollConsumerScheduler</code>
         * type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder scheduler(String scheduler) {
            setProperty("scheduler", scheduler);
            return this;
        }
        /**
         * To configure additional properties when using a custom scheduler or
         * any of the Quartz2, Spring based scheduler.
         * The option is a <code>java.util.Map&lt;java.lang.String,
         * java.lang.Object&gt;</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder schedulerProperties(
                Map<String, Object> schedulerProperties) {
            setProperty("schedulerProperties", schedulerProperties);
            return this;
        }
        /**
         * To configure additional properties when using a custom scheduler or
         * any of the Quartz2, Spring based scheduler.
         * The option will be converted to a
         * <code>java.util.Map&lt;java.lang.String, java.lang.Object&gt;</code>
         * type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder schedulerProperties(
                String schedulerProperties) {
            setProperty("schedulerProperties", schedulerProperties);
            return this;
        }
        /**
         * Whether the scheduler should be auto started.
         * The option is a <code>boolean</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder startScheduler(
                boolean startScheduler) {
            setProperty("startScheduler", startScheduler);
            return this;
        }
        /**
         * Whether the scheduler should be auto started.
         * The option will be converted to a <code>boolean</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder startScheduler(String startScheduler) {
            setProperty("startScheduler", startScheduler);
            return this;
        }
        /**
         * Time unit for initialDelay and delay options.
         * The option is a <code>java.util.concurrent.TimeUnit</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder timeUnit(TimeUnit timeUnit) {
            setProperty("timeUnit", timeUnit);
            return this;
        }
        /**
         * Time unit for initialDelay and delay options.
         * The option will be converted to a
         * <code>java.util.concurrent.TimeUnit</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder timeUnit(String timeUnit) {
            setProperty("timeUnit", timeUnit);
            return this;
        }
        /**
         * Controls if fixed delay or fixed rate is used. See
         * ScheduledExecutorService in JDK for details.
         * The option is a <code>boolean</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder useFixedDelay(boolean useFixedDelay) {
            setProperty("useFixedDelay", useFixedDelay);
            return this;
        }
        /**
         * Controls if fixed delay or fixed rate is used. See
         * ScheduledExecutorService in JDK for details.
         * The option will be converted to a <code>boolean</code> type.
         * @group scheduler
         */
        default HdfsEndpointConsumerBuilder useFixedDelay(String useFixedDelay) {
            setProperty("useFixedDelay", useFixedDelay);
            return this;
        }
    }

    /**
     * Advanced builder for endpoint consumers for the HDFS2 component.
     */
    public interface AdvancedHdfsEndpointConsumerBuilder
            extends
                EndpointConsumerBuilder {
        default HdfsEndpointConsumerBuilder basic() {
            return (HdfsEndpointConsumerBuilder) this;
        }
        /**
         * To let the consumer use a custom ExceptionHandler. Notice if the
         * option bridgeErrorHandler is enabled then this option is not in use.
         * By default the consumer will deal with exceptions, that will be
         * logged at WARN or ERROR level and ignored.
         * The option is a <code>org.apache.camel.spi.ExceptionHandler</code>
         * type.
         * @group consumer (advanced)
         */
        default AdvancedHdfsEndpointConsumerBuilder exceptionHandler(
                ExceptionHandler exceptionHandler) {
            setProperty("exceptionHandler", exceptionHandler);
            return this;
        }
        /**
         * To let the consumer use a custom ExceptionHandler. Notice if the
         * option bridgeErrorHandler is enabled then this option is not in use.
         * By default the consumer will deal with exceptions, that will be
         * logged at WARN or ERROR level and ignored.
         * The option will be converted to a
         * <code>org.apache.camel.spi.ExceptionHandler</code> type.
         * @group consumer (advanced)
         */
        default AdvancedHdfsEndpointConsumerBuilder exceptionHandler(
                String exceptionHandler) {
            setProperty("exceptionHandler", exceptionHandler);
            return this;
        }
        /**
         * Sets the exchange pattern when the consumer creates an exchange.
         * The option is a <code>org.apache.camel.ExchangePattern</code> type.
         * @group consumer (advanced)
         */
        default AdvancedHdfsEndpointConsumerBuilder exchangePattern(
                ExchangePattern exchangePattern) {
            setProperty("exchangePattern", exchangePattern);
            return this;
        }
        /**
         * Sets the exchange pattern when the consumer creates an exchange.
         * The option will be converted to a
         * <code>org.apache.camel.ExchangePattern</code> type.
         * @group consumer (advanced)
         */
        default AdvancedHdfsEndpointConsumerBuilder exchangePattern(
                String exchangePattern) {
            setProperty("exchangePattern", exchangePattern);
            return this;
        }
        /**
         * A pluggable org.apache.camel.PollingConsumerPollingStrategy allowing
         * you to provide your custom implementation to control error handling
         * usually occurred during the poll operation before an Exchange have
         * been created and being routed in Camel.
         * The option is a
         * <code>org.apache.camel.spi.PollingConsumerPollStrategy</code> type.
         * @group consumer (advanced)
         */
        default AdvancedHdfsEndpointConsumerBuilder pollStrategy(
                PollingConsumerPollStrategy pollStrategy) {
            setProperty("pollStrategy", pollStrategy);
            return this;
        }
        /**
         * A pluggable org.apache.camel.PollingConsumerPollingStrategy allowing
         * you to provide your custom implementation to control error handling
         * usually occurred during the poll operation before an Exchange have
         * been created and being routed in Camel.
         * The option will be converted to a
         * <code>org.apache.camel.spi.PollingConsumerPollStrategy</code> type.
         * @group consumer (advanced)
         */
        default AdvancedHdfsEndpointConsumerBuilder pollStrategy(
                String pollStrategy) {
            setProperty("pollStrategy", pollStrategy);
            return this;
        }
        /**
         * Whether the endpoint should use basic property binding (Camel 2.x) or
         * the newer property binding with additional capabilities.
         * The option is a <code>boolean</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder basicPropertyBinding(
                boolean basicPropertyBinding) {
            setProperty("basicPropertyBinding", basicPropertyBinding);
            return this;
        }
        /**
         * Whether the endpoint should use basic property binding (Camel 2.x) or
         * the newer property binding with additional capabilities.
         * The option will be converted to a <code>boolean</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder basicPropertyBinding(
                String basicPropertyBinding) {
            setProperty("basicPropertyBinding", basicPropertyBinding);
            return this;
        }
        /**
         * The size of the HDFS blocks.
         * The option is a <code>long</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder blockSize(long blockSize) {
            setProperty("blockSize", blockSize);
            return this;
        }
        /**
         * The size of the HDFS blocks.
         * The option will be converted to a <code>long</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder blockSize(String blockSize) {
            setProperty("blockSize", blockSize);
            return this;
        }
        /**
         * The buffer size used by HDFS.
         * The option is a <code>int</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder bufferSize(int bufferSize) {
            setProperty("bufferSize", bufferSize);
            return this;
        }
        /**
         * The buffer size used by HDFS.
         * The option will be converted to a <code>int</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder bufferSize(String bufferSize) {
            setProperty("bufferSize", bufferSize);
            return this;
        }
        /**
         * How often (time in millis) in to run the idle checker background
         * task. This option is only in use if the splitter strategy is IDLE.
         * The option is a <code>int</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder checkIdleInterval(
                int checkIdleInterval) {
            setProperty("checkIdleInterval", checkIdleInterval);
            return this;
        }
        /**
         * How often (time in millis) in to run the idle checker background
         * task. This option is only in use if the splitter strategy is IDLE.
         * The option will be converted to a <code>int</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder checkIdleInterval(
                String checkIdleInterval) {
            setProperty("checkIdleInterval", checkIdleInterval);
            return this;
        }
        /**
         * When reading a normal file, this is split into chunks producing a
         * message per chunk.
         * The option is a <code>int</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder chunkSize(int chunkSize) {
            setProperty("chunkSize", chunkSize);
            return this;
        }
        /**
         * When reading a normal file, this is split into chunks producing a
         * message per chunk.
         * The option will be converted to a <code>int</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder chunkSize(String chunkSize) {
            setProperty("chunkSize", chunkSize);
            return this;
        }
        /**
         * The compression codec to use.
         * The option is a
         * <code>org.apache.camel.component.hdfs2.HdfsCompressionCodec</code>
         * type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder compressionCodec(
                HdfsCompressionCodec compressionCodec) {
            setProperty("compressionCodec", compressionCodec);
            return this;
        }
        /**
         * The compression codec to use.
         * The option will be converted to a
         * <code>org.apache.camel.component.hdfs2.HdfsCompressionCodec</code>
         * type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder compressionCodec(
                String compressionCodec) {
            setProperty("compressionCodec", compressionCodec);
            return this;
        }
        /**
         * The compression type to use (is default not in use).
         * The option is a
         * <code>org.apache.hadoop.io.SequenceFile$CompressionType</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder compressionType(
                CompressionType compressionType) {
            setProperty("compressionType", compressionType);
            return this;
        }
        /**
         * The compression type to use (is default not in use).
         * The option will be converted to a
         * <code>org.apache.hadoop.io.SequenceFile$CompressionType</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder compressionType(
                String compressionType) {
            setProperty("compressionType", compressionType);
            return this;
        }
        /**
         * When a file is opened for reading/writing the file is renamed with
         * this suffix to avoid to read it during the writing phase.
         * The option is a <code>java.lang.String</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder openedSuffix(
                String openedSuffix) {
            setProperty("openedSuffix", openedSuffix);
            return this;
        }
        /**
         * Once the file has been read is renamed with this suffix to avoid to
         * read it again.
         * The option is a <code>java.lang.String</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder readSuffix(String readSuffix) {
            setProperty("readSuffix", readSuffix);
            return this;
        }
        /**
         * The HDFS replication factor.
         * The option is a <code>short</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder replication(
                short replication) {
            setProperty("replication", replication);
            return this;
        }
        /**
         * The HDFS replication factor.
         * The option will be converted to a <code>short</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder replication(
                String replication) {
            setProperty("replication", replication);
            return this;
        }
        /**
         * In the current version of Hadoop opening a file in append mode is
         * disabled since it's not very reliable. So, for the moment, it's only
         * possible to create new files. The Camel HDFS endpoint tries to solve
         * this problem in this way: If the split strategy option has been
         * defined, the hdfs path will be used as a directory and files will be
         * created using the configured UuidGenerator. Every time a splitting
         * condition is met, a new file is created. The splitStrategy option is
         * defined as a string with the following syntax:
         * splitStrategy=ST:value,ST:value,... where ST can be: BYTES a new file
         * is created, and the old is closed when the number of written bytes is
         * more than value MESSAGES a new file is created, and the old is closed
         * when the number of written messages is more than value IDLE a new
         * file is created, and the old is closed when no writing happened in
         * the last value milliseconds.
         * The option is a <code>java.lang.String</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder splitStrategy(
                String splitStrategy) {
            setProperty("splitStrategy", splitStrategy);
            return this;
        }
        /**
         * Sets whether synchronous processing should be strictly used, or Camel
         * is allowed to use asynchronous processing (if supported).
         * The option is a <code>boolean</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder synchronous(
                boolean synchronous) {
            setProperty("synchronous", synchronous);
            return this;
        }
        /**
         * Sets whether synchronous processing should be strictly used, or Camel
         * is allowed to use asynchronous processing (if supported).
         * The option will be converted to a <code>boolean</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointConsumerBuilder synchronous(
                String synchronous) {
            setProperty("synchronous", synchronous);
            return this;
        }
    }

    /**
     * Builder for endpoint producers for the HDFS2 component.
     */
    public interface HdfsEndpointProducerBuilder
            extends
                EndpointProducerBuilder {
        default AdvancedHdfsEndpointProducerBuilder advanced() {
            return (AdvancedHdfsEndpointProducerBuilder) this;
        }
        /**
         * HDFS host to use.
         * The option is a <code>java.lang.String</code> type.
         * @group common
         */
        default HdfsEndpointProducerBuilder hostName(String hostName) {
            setProperty("hostName", hostName);
            return this;
        }
        /**
         * HDFS port to use.
         * The option is a <code>int</code> type.
         * @group common
         */
        default HdfsEndpointProducerBuilder port(int port) {
            setProperty("port", port);
            return this;
        }
        /**
         * HDFS port to use.
         * The option will be converted to a <code>int</code> type.
         * @group common
         */
        default HdfsEndpointProducerBuilder port(String port) {
            setProperty("port", port);
            return this;
        }
        /**
         * The directory path to use.
         * The option is a <code>java.lang.String</code> type.
         * @group common
         */
        default HdfsEndpointProducerBuilder path(String path) {
            setProperty("path", path);
            return this;
        }
        /**
         * Whether to connect to the HDFS file system on starting the
         * producer/consumer. If false then the connection is created on-demand.
         * Notice that HDFS may take up till 15 minutes to establish a
         * connection, as it has hardcoded 45 x 20 sec redelivery. By setting
         * this option to false allows your application to startup, and not
         * block for up till 15 minutes.
         * The option is a <code>boolean</code> type.
         * @group common
         */
        default HdfsEndpointProducerBuilder connectOnStartup(
                boolean connectOnStartup) {
            setProperty("connectOnStartup", connectOnStartup);
            return this;
        }
        /**
         * Whether to connect to the HDFS file system on starting the
         * producer/consumer. If false then the connection is created on-demand.
         * Notice that HDFS may take up till 15 minutes to establish a
         * connection, as it has hardcoded 45 x 20 sec redelivery. By setting
         * this option to false allows your application to startup, and not
         * block for up till 15 minutes.
         * The option will be converted to a <code>boolean</code> type.
         * @group common
         */
        default HdfsEndpointProducerBuilder connectOnStartup(
                String connectOnStartup) {
            setProperty("connectOnStartup", connectOnStartup);
            return this;
        }
        /**
         * Set to LOCAL to not use HDFS but local java.io.File instead.
         * The option is a
         * <code>org.apache.camel.component.hdfs2.HdfsFileSystemType</code>
         * type.
         * @group common
         */
        default HdfsEndpointProducerBuilder fileSystemType(
                HdfsFileSystemType fileSystemType) {
            setProperty("fileSystemType", fileSystemType);
            return this;
        }
        /**
         * Set to LOCAL to not use HDFS but local java.io.File instead.
         * The option will be converted to a
         * <code>org.apache.camel.component.hdfs2.HdfsFileSystemType</code>
         * type.
         * @group common
         */
        default HdfsEndpointProducerBuilder fileSystemType(String fileSystemType) {
            setProperty("fileSystemType", fileSystemType);
            return this;
        }
        /**
         * The file type to use. For more details see Hadoop HDFS documentation
         * about the various files types.
         * The option is a
         * <code>org.apache.camel.component.hdfs2.HdfsFileType</code> type.
         * @group common
         */
        default HdfsEndpointProducerBuilder fileType(HdfsFileType fileType) {
            setProperty("fileType", fileType);
            return this;
        }
        /**
         * The file type to use. For more details see Hadoop HDFS documentation
         * about the various files types.
         * The option will be converted to a
         * <code>org.apache.camel.component.hdfs2.HdfsFileType</code> type.
         * @group common
         */
        default HdfsEndpointProducerBuilder fileType(String fileType) {
            setProperty("fileType", fileType);
            return this;
        }
        /**
         * The type for the key in case of sequence or map files.
         * The option is a
         * <code>org.apache.camel.component.hdfs2.WritableType</code> type.
         * @group common
         */
        default HdfsEndpointProducerBuilder keyType(WritableType keyType) {
            setProperty("keyType", keyType);
            return this;
        }
        /**
         * The type for the key in case of sequence or map files.
         * The option will be converted to a
         * <code>org.apache.camel.component.hdfs2.WritableType</code> type.
         * @group common
         */
        default HdfsEndpointProducerBuilder keyType(String keyType) {
            setProperty("keyType", keyType);
            return this;
        }
        /**
         * The file owner must match this owner for the consumer to pickup the
         * file. Otherwise the file is skipped.
         * The option is a <code>java.lang.String</code> type.
         * @group common
         */
        default HdfsEndpointProducerBuilder owner(String owner) {
            setProperty("owner", owner);
            return this;
        }
        /**
         * The type for the key in case of sequence or map files.
         * The option is a
         * <code>org.apache.camel.component.hdfs2.WritableType</code> type.
         * @group common
         */
        default HdfsEndpointProducerBuilder valueType(WritableType valueType) {
            setProperty("valueType", valueType);
            return this;
        }
        /**
         * The type for the key in case of sequence or map files.
         * The option will be converted to a
         * <code>org.apache.camel.component.hdfs2.WritableType</code> type.
         * @group common
         */
        default HdfsEndpointProducerBuilder valueType(String valueType) {
            setProperty("valueType", valueType);
            return this;
        }
        /**
         * Append to existing file. Notice that not all HDFS file systems
         * support the append option.
         * The option is a <code>boolean</code> type.
         * @group producer
         */
        default HdfsEndpointProducerBuilder append(boolean append) {
            setProperty("append", append);
            return this;
        }
        /**
         * Append to existing file. Notice that not all HDFS file systems
         * support the append option.
         * The option will be converted to a <code>boolean</code> type.
         * @group producer
         */
        default HdfsEndpointProducerBuilder append(String append) {
            setProperty("append", append);
            return this;
        }
        /**
         * Whether the producer should be started lazy (on the first message).
         * By starting lazy you can use this to allow CamelContext and routes to
         * startup in situations where a producer may otherwise fail during
         * starting and cause the route to fail being started. By deferring this
         * startup to be lazy then the startup failure can be handled during
         * routing messages via Camel's routing error handlers. Beware that when
         * the first message is processed then creating and starting the
         * producer may take a little time and prolong the total processing time
         * of the processing.
         * The option is a <code>boolean</code> type.
         * @group producer
         */
        default HdfsEndpointProducerBuilder lazyStartProducer(
                boolean lazyStartProducer) {
            setProperty("lazyStartProducer", lazyStartProducer);
            return this;
        }
        /**
         * Whether the producer should be started lazy (on the first message).
         * By starting lazy you can use this to allow CamelContext and routes to
         * startup in situations where a producer may otherwise fail during
         * starting and cause the route to fail being started. By deferring this
         * startup to be lazy then the startup failure can be handled during
         * routing messages via Camel's routing error handlers. Beware that when
         * the first message is processed then creating and starting the
         * producer may take a little time and prolong the total processing time
         * of the processing.
         * The option will be converted to a <code>boolean</code> type.
         * @group producer
         */
        default HdfsEndpointProducerBuilder lazyStartProducer(
                String lazyStartProducer) {
            setProperty("lazyStartProducer", lazyStartProducer);
            return this;
        }
        /**
         * Whether to overwrite existing files with the same name.
         * The option is a <code>boolean</code> type.
         * @group producer
         */
        default HdfsEndpointProducerBuilder overwrite(boolean overwrite) {
            setProperty("overwrite", overwrite);
            return this;
        }
        /**
         * Whether to overwrite existing files with the same name.
         * The option will be converted to a <code>boolean</code> type.
         * @group producer
         */
        default HdfsEndpointProducerBuilder overwrite(String overwrite) {
            setProperty("overwrite", overwrite);
            return this;
        }
    }

    /**
     * Advanced builder for endpoint producers for the HDFS2 component.
     */
    public interface AdvancedHdfsEndpointProducerBuilder
            extends
                EndpointProducerBuilder {
        default HdfsEndpointProducerBuilder basic() {
            return (HdfsEndpointProducerBuilder) this;
        }
        /**
         * Whether the endpoint should use basic property binding (Camel 2.x) or
         * the newer property binding with additional capabilities.
         * The option is a <code>boolean</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder basicPropertyBinding(
                boolean basicPropertyBinding) {
            setProperty("basicPropertyBinding", basicPropertyBinding);
            return this;
        }
        /**
         * Whether the endpoint should use basic property binding (Camel 2.x) or
         * the newer property binding with additional capabilities.
         * The option will be converted to a <code>boolean</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder basicPropertyBinding(
                String basicPropertyBinding) {
            setProperty("basicPropertyBinding", basicPropertyBinding);
            return this;
        }
        /**
         * The size of the HDFS blocks.
         * The option is a <code>long</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder blockSize(long blockSize) {
            setProperty("blockSize", blockSize);
            return this;
        }
        /**
         * The size of the HDFS blocks.
         * The option will be converted to a <code>long</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder blockSize(String blockSize) {
            setProperty("blockSize", blockSize);
            return this;
        }
        /**
         * The buffer size used by HDFS.
         * The option is a <code>int</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder bufferSize(int bufferSize) {
            setProperty("bufferSize", bufferSize);
            return this;
        }
        /**
         * The buffer size used by HDFS.
         * The option will be converted to a <code>int</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder bufferSize(String bufferSize) {
            setProperty("bufferSize", bufferSize);
            return this;
        }
        /**
         * How often (time in millis) in to run the idle checker background
         * task. This option is only in use if the splitter strategy is IDLE.
         * The option is a <code>int</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder checkIdleInterval(
                int checkIdleInterval) {
            setProperty("checkIdleInterval", checkIdleInterval);
            return this;
        }
        /**
         * How often (time in millis) in to run the idle checker background
         * task. This option is only in use if the splitter strategy is IDLE.
         * The option will be converted to a <code>int</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder checkIdleInterval(
                String checkIdleInterval) {
            setProperty("checkIdleInterval", checkIdleInterval);
            return this;
        }
        /**
         * When reading a normal file, this is split into chunks producing a
         * message per chunk.
         * The option is a <code>int</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder chunkSize(int chunkSize) {
            setProperty("chunkSize", chunkSize);
            return this;
        }
        /**
         * When reading a normal file, this is split into chunks producing a
         * message per chunk.
         * The option will be converted to a <code>int</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder chunkSize(String chunkSize) {
            setProperty("chunkSize", chunkSize);
            return this;
        }
        /**
         * The compression codec to use.
         * The option is a
         * <code>org.apache.camel.component.hdfs2.HdfsCompressionCodec</code>
         * type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder compressionCodec(
                HdfsCompressionCodec compressionCodec) {
            setProperty("compressionCodec", compressionCodec);
            return this;
        }
        /**
         * The compression codec to use.
         * The option will be converted to a
         * <code>org.apache.camel.component.hdfs2.HdfsCompressionCodec</code>
         * type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder compressionCodec(
                String compressionCodec) {
            setProperty("compressionCodec", compressionCodec);
            return this;
        }
        /**
         * The compression type to use (is default not in use).
         * The option is a
         * <code>org.apache.hadoop.io.SequenceFile$CompressionType</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder compressionType(
                CompressionType compressionType) {
            setProperty("compressionType", compressionType);
            return this;
        }
        /**
         * The compression type to use (is default not in use).
         * The option will be converted to a
         * <code>org.apache.hadoop.io.SequenceFile$CompressionType</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder compressionType(
                String compressionType) {
            setProperty("compressionType", compressionType);
            return this;
        }
        /**
         * When a file is opened for reading/writing the file is renamed with
         * this suffix to avoid to read it during the writing phase.
         * The option is a <code>java.lang.String</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder openedSuffix(
                String openedSuffix) {
            setProperty("openedSuffix", openedSuffix);
            return this;
        }
        /**
         * Once the file has been read is renamed with this suffix to avoid to
         * read it again.
         * The option is a <code>java.lang.String</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder readSuffix(String readSuffix) {
            setProperty("readSuffix", readSuffix);
            return this;
        }
        /**
         * The HDFS replication factor.
         * The option is a <code>short</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder replication(
                short replication) {
            setProperty("replication", replication);
            return this;
        }
        /**
         * The HDFS replication factor.
         * The option will be converted to a <code>short</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder replication(
                String replication) {
            setProperty("replication", replication);
            return this;
        }
        /**
         * In the current version of Hadoop opening a file in append mode is
         * disabled since it's not very reliable. So, for the moment, it's only
         * possible to create new files. The Camel HDFS endpoint tries to solve
         * this problem in this way: If the split strategy option has been
         * defined, the hdfs path will be used as a directory and files will be
         * created using the configured UuidGenerator. Every time a splitting
         * condition is met, a new file is created. The splitStrategy option is
         * defined as a string with the following syntax:
         * splitStrategy=ST:value,ST:value,... where ST can be: BYTES a new file
         * is created, and the old is closed when the number of written bytes is
         * more than value MESSAGES a new file is created, and the old is closed
         * when the number of written messages is more than value IDLE a new
         * file is created, and the old is closed when no writing happened in
         * the last value milliseconds.
         * The option is a <code>java.lang.String</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder splitStrategy(
                String splitStrategy) {
            setProperty("splitStrategy", splitStrategy);
            return this;
        }
        /**
         * Sets whether synchronous processing should be strictly used, or Camel
         * is allowed to use asynchronous processing (if supported).
         * The option is a <code>boolean</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder synchronous(
                boolean synchronous) {
            setProperty("synchronous", synchronous);
            return this;
        }
        /**
         * Sets whether synchronous processing should be strictly used, or Camel
         * is allowed to use asynchronous processing (if supported).
         * The option will be converted to a <code>boolean</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointProducerBuilder synchronous(
                String synchronous) {
            setProperty("synchronous", synchronous);
            return this;
        }
    }

    /**
     * Builder for endpoint for the HDFS2 component.
     */
    public interface HdfsEndpointBuilder
            extends
                HdfsEndpointConsumerBuilder, HdfsEndpointProducerBuilder {
        default AdvancedHdfsEndpointBuilder advanced() {
            return (AdvancedHdfsEndpointBuilder) this;
        }
        /**
         * HDFS host to use.
         * The option is a <code>java.lang.String</code> type.
         * @group common
         */
        default HdfsEndpointBuilder hostName(String hostName) {
            setProperty("hostName", hostName);
            return this;
        }
        /**
         * HDFS port to use.
         * The option is a <code>int</code> type.
         * @group common
         */
        default HdfsEndpointBuilder port(int port) {
            setProperty("port", port);
            return this;
        }
        /**
         * HDFS port to use.
         * The option will be converted to a <code>int</code> type.
         * @group common
         */
        default HdfsEndpointBuilder port(String port) {
            setProperty("port", port);
            return this;
        }
        /**
         * The directory path to use.
         * The option is a <code>java.lang.String</code> type.
         * @group common
         */
        default HdfsEndpointBuilder path(String path) {
            setProperty("path", path);
            return this;
        }
        /**
         * Whether to connect to the HDFS file system on starting the
         * producer/consumer. If false then the connection is created on-demand.
         * Notice that HDFS may take up till 15 minutes to establish a
         * connection, as it has hardcoded 45 x 20 sec redelivery. By setting
         * this option to false allows your application to startup, and not
         * block for up till 15 minutes.
         * The option is a <code>boolean</code> type.
         * @group common
         */
        default HdfsEndpointBuilder connectOnStartup(boolean connectOnStartup) {
            setProperty("connectOnStartup", connectOnStartup);
            return this;
        }
        /**
         * Whether to connect to the HDFS file system on starting the
         * producer/consumer. If false then the connection is created on-demand.
         * Notice that HDFS may take up till 15 minutes to establish a
         * connection, as it has hardcoded 45 x 20 sec redelivery. By setting
         * this option to false allows your application to startup, and not
         * block for up till 15 minutes.
         * The option will be converted to a <code>boolean</code> type.
         * @group common
         */
        default HdfsEndpointBuilder connectOnStartup(String connectOnStartup) {
            setProperty("connectOnStartup", connectOnStartup);
            return this;
        }
        /**
         * Set to LOCAL to not use HDFS but local java.io.File instead.
         * The option is a
         * <code>org.apache.camel.component.hdfs2.HdfsFileSystemType</code>
         * type.
         * @group common
         */
        default HdfsEndpointBuilder fileSystemType(
                HdfsFileSystemType fileSystemType) {
            setProperty("fileSystemType", fileSystemType);
            return this;
        }
        /**
         * Set to LOCAL to not use HDFS but local java.io.File instead.
         * The option will be converted to a
         * <code>org.apache.camel.component.hdfs2.HdfsFileSystemType</code>
         * type.
         * @group common
         */
        default HdfsEndpointBuilder fileSystemType(String fileSystemType) {
            setProperty("fileSystemType", fileSystemType);
            return this;
        }
        /**
         * The file type to use. For more details see Hadoop HDFS documentation
         * about the various files types.
         * The option is a
         * <code>org.apache.camel.component.hdfs2.HdfsFileType</code> type.
         * @group common
         */
        default HdfsEndpointBuilder fileType(HdfsFileType fileType) {
            setProperty("fileType", fileType);
            return this;
        }
        /**
         * The file type to use. For more details see Hadoop HDFS documentation
         * about the various files types.
         * The option will be converted to a
         * <code>org.apache.camel.component.hdfs2.HdfsFileType</code> type.
         * @group common
         */
        default HdfsEndpointBuilder fileType(String fileType) {
            setProperty("fileType", fileType);
            return this;
        }
        /**
         * The type for the key in case of sequence or map files.
         * The option is a
         * <code>org.apache.camel.component.hdfs2.WritableType</code> type.
         * @group common
         */
        default HdfsEndpointBuilder keyType(WritableType keyType) {
            setProperty("keyType", keyType);
            return this;
        }
        /**
         * The type for the key in case of sequence or map files.
         * The option will be converted to a
         * <code>org.apache.camel.component.hdfs2.WritableType</code> type.
         * @group common
         */
        default HdfsEndpointBuilder keyType(String keyType) {
            setProperty("keyType", keyType);
            return this;
        }
        /**
         * The file owner must match this owner for the consumer to pickup the
         * file. Otherwise the file is skipped.
         * The option is a <code>java.lang.String</code> type.
         * @group common
         */
        default HdfsEndpointBuilder owner(String owner) {
            setProperty("owner", owner);
            return this;
        }
        /**
         * The type for the key in case of sequence or map files.
         * The option is a
         * <code>org.apache.camel.component.hdfs2.WritableType</code> type.
         * @group common
         */
        default HdfsEndpointBuilder valueType(WritableType valueType) {
            setProperty("valueType", valueType);
            return this;
        }
        /**
         * The type for the key in case of sequence or map files.
         * The option will be converted to a
         * <code>org.apache.camel.component.hdfs2.WritableType</code> type.
         * @group common
         */
        default HdfsEndpointBuilder valueType(String valueType) {
            setProperty("valueType", valueType);
            return this;
        }
    }

    /**
     * Advanced builder for endpoint for the HDFS2 component.
     */
    public interface AdvancedHdfsEndpointBuilder
            extends
                AdvancedHdfsEndpointConsumerBuilder, AdvancedHdfsEndpointProducerBuilder {
        default HdfsEndpointBuilder basic() {
            return (HdfsEndpointBuilder) this;
        }
        /**
         * Whether the endpoint should use basic property binding (Camel 2.x) or
         * the newer property binding with additional capabilities.
         * The option is a <code>boolean</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder basicPropertyBinding(
                boolean basicPropertyBinding) {
            setProperty("basicPropertyBinding", basicPropertyBinding);
            return this;
        }
        /**
         * Whether the endpoint should use basic property binding (Camel 2.x) or
         * the newer property binding with additional capabilities.
         * The option will be converted to a <code>boolean</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder basicPropertyBinding(
                String basicPropertyBinding) {
            setProperty("basicPropertyBinding", basicPropertyBinding);
            return this;
        }
        /**
         * The size of the HDFS blocks.
         * The option is a <code>long</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder blockSize(long blockSize) {
            setProperty("blockSize", blockSize);
            return this;
        }
        /**
         * The size of the HDFS blocks.
         * The option will be converted to a <code>long</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder blockSize(String blockSize) {
            setProperty("blockSize", blockSize);
            return this;
        }
        /**
         * The buffer size used by HDFS.
         * The option is a <code>int</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder bufferSize(int bufferSize) {
            setProperty("bufferSize", bufferSize);
            return this;
        }
        /**
         * The buffer size used by HDFS.
         * The option will be converted to a <code>int</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder bufferSize(String bufferSize) {
            setProperty("bufferSize", bufferSize);
            return this;
        }
        /**
         * How often (time in millis) in to run the idle checker background
         * task. This option is only in use if the splitter strategy is IDLE.
         * The option is a <code>int</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder checkIdleInterval(
                int checkIdleInterval) {
            setProperty("checkIdleInterval", checkIdleInterval);
            return this;
        }
        /**
         * How often (time in millis) in to run the idle checker background
         * task. This option is only in use if the splitter strategy is IDLE.
         * The option will be converted to a <code>int</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder checkIdleInterval(
                String checkIdleInterval) {
            setProperty("checkIdleInterval", checkIdleInterval);
            return this;
        }
        /**
         * When reading a normal file, this is split into chunks producing a
         * message per chunk.
         * The option is a <code>int</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder chunkSize(int chunkSize) {
            setProperty("chunkSize", chunkSize);
            return this;
        }
        /**
         * When reading a normal file, this is split into chunks producing a
         * message per chunk.
         * The option will be converted to a <code>int</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder chunkSize(String chunkSize) {
            setProperty("chunkSize", chunkSize);
            return this;
        }
        /**
         * The compression codec to use.
         * The option is a
         * <code>org.apache.camel.component.hdfs2.HdfsCompressionCodec</code>
         * type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder compressionCodec(
                HdfsCompressionCodec compressionCodec) {
            setProperty("compressionCodec", compressionCodec);
            return this;
        }
        /**
         * The compression codec to use.
         * The option will be converted to a
         * <code>org.apache.camel.component.hdfs2.HdfsCompressionCodec</code>
         * type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder compressionCodec(
                String compressionCodec) {
            setProperty("compressionCodec", compressionCodec);
            return this;
        }
        /**
         * The compression type to use (is default not in use).
         * The option is a
         * <code>org.apache.hadoop.io.SequenceFile$CompressionType</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder compressionType(
                CompressionType compressionType) {
            setProperty("compressionType", compressionType);
            return this;
        }
        /**
         * The compression type to use (is default not in use).
         * The option will be converted to a
         * <code>org.apache.hadoop.io.SequenceFile$CompressionType</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder compressionType(
                String compressionType) {
            setProperty("compressionType", compressionType);
            return this;
        }
        /**
         * When a file is opened for reading/writing the file is renamed with
         * this suffix to avoid to read it during the writing phase.
         * The option is a <code>java.lang.String</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder openedSuffix(String openedSuffix) {
            setProperty("openedSuffix", openedSuffix);
            return this;
        }
        /**
         * Once the file has been read is renamed with this suffix to avoid to
         * read it again.
         * The option is a <code>java.lang.String</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder readSuffix(String readSuffix) {
            setProperty("readSuffix", readSuffix);
            return this;
        }
        /**
         * The HDFS replication factor.
         * The option is a <code>short</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder replication(short replication) {
            setProperty("replication", replication);
            return this;
        }
        /**
         * The HDFS replication factor.
         * The option will be converted to a <code>short</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder replication(String replication) {
            setProperty("replication", replication);
            return this;
        }
        /**
         * In the current version of Hadoop opening a file in append mode is
         * disabled since it's not very reliable. So, for the moment, it's only
         * possible to create new files. The Camel HDFS endpoint tries to solve
         * this problem in this way: If the split strategy option has been
         * defined, the hdfs path will be used as a directory and files will be
         * created using the configured UuidGenerator. Every time a splitting
         * condition is met, a new file is created. The splitStrategy option is
         * defined as a string with the following syntax:
         * splitStrategy=ST:value,ST:value,... where ST can be: BYTES a new file
         * is created, and the old is closed when the number of written bytes is
         * more than value MESSAGES a new file is created, and the old is closed
         * when the number of written messages is more than value IDLE a new
         * file is created, and the old is closed when no writing happened in
         * the last value milliseconds.
         * The option is a <code>java.lang.String</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder splitStrategy(String splitStrategy) {
            setProperty("splitStrategy", splitStrategy);
            return this;
        }
        /**
         * Sets whether synchronous processing should be strictly used, or Camel
         * is allowed to use asynchronous processing (if supported).
         * The option is a <code>boolean</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder synchronous(boolean synchronous) {
            setProperty("synchronous", synchronous);
            return this;
        }
        /**
         * Sets whether synchronous processing should be strictly used, or Camel
         * is allowed to use asynchronous processing (if supported).
         * The option will be converted to a <code>boolean</code> type.
         * @group advanced
         */
        default AdvancedHdfsEndpointBuilder synchronous(String synchronous) {
            setProperty("synchronous", synchronous);
            return this;
        }
    }

    /**
     * Proxy enum for
     * <code>org.apache.camel.component.hdfs2.HdfsFileSystemType</code> enum.
     */
    enum HdfsFileSystemType {
        LOCAL,
        HDFS;
    }

    /**
     * Proxy enum for <code>org.apache.camel.component.hdfs2.HdfsFileType</code>
     * enum.
     */
    enum HdfsFileType {
        NORMAL_FILE,
        SEQUENCE_FILE,
        MAP_FILE,
        BLOOMMAP_FILE,
        ARRAY_FILE;
    }

    /**
     * Proxy enum for <code>org.apache.camel.component.hdfs2.WritableType</code>
     * enum.
     */
    enum WritableType {
        NULL,
        BOOLEAN,
        BYTE,
        INT,
        FLOAT,
        LONG,
        DOUBLE,
        TEXT,
        BYTES;
    }

    /**
     * Proxy enum for
     * <code>org.apache.camel.component.hdfs2.HdfsCompressionCodec</code> enum.
     */
    enum HdfsCompressionCodec {
        DEFAULT,
        GZIP,
        BZIP2;
    }

    /**
     * Proxy enum for
     * <code>org.apache.hadoop.io.SequenceFile$CompressionType</code> enum.
     */
    enum CompressionType {
        NONE,
        RECORD,
        BLOCK;
    }
    /**
     * For reading/writing from/to an HDFS filesystem using Hadoop 2.x.
     * Maven coordinates: org.apache.camel:camel-hdfs2
     */
    default HdfsEndpointBuilder hdfs(String path) {
        class HdfsEndpointBuilderImpl extends AbstractEndpointBuilder implements HdfsEndpointBuilder, AdvancedHdfsEndpointBuilder {
            public HdfsEndpointBuilderImpl(String path) {
                super("hdfs", path);
            }
        }
        return new HdfsEndpointBuilderImpl(path);
    }
}