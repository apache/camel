[[debezium-component]]
= Debezium Component

*Available as of Camel version 3.0*

The Debezium component is wrapper around https://debezium.io/[Debezium] using https://debezium.io/documentation/reference/0.9/operations/embedded.html[Debezium Embedded], which enables Change Data Capture from various databases using Debezium without the need for Kafka or Kafka Connect.

*Note on handling failures:* Per https://debezium.io/documentation/reference/0.9/operations/embedded.html#_handling_failures[Debezium Embedded Engine] documentation, the engines is actively recording source offsets and periodically flushes these offsets to a persistent storage, so when the application is restarted or crashed, the engine will resume from the last recorded offset.
Thus, at normal operation, your downstream routes will receive each event exactly once, however in case of an application crash (not having a graceful shutdown), the application will resume from the last recorded offset,
which may result in receiving duplicate events immediately after the restart. Therefore, your downstream routes should be tolerant enough of such case and deduplicate events if needed.

Maven users will need to add the following dependency to their `pom.xml`
for this component.

[source,xml]
------------------------------------------------------------
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-debezium</artifactId>
    <version>x.x.x</version>
    <!-- use the same version as your Camel core version -->
</dependency>
------------------------------------------------------------

== URI format

[source,java]
---------------------------
debezium:connector-type[?options]

---------------------------

== Supported Debezium Connectors
- https://debezium.io/documentation/reference/0.9/connectors/mysql.html[MySQL].

*Note:* Other Debezium connectors are _not_ supported at the moment.


*Note:* Due to licensing issues, you will need to add the dependency for `mysql-conenctor-java` if you are using MySQL connector, just add the following to your POM file:
[source,xml]
------------------------------------------------------------
<dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
    <version>8.0.15</version>
</dependency>
------------------------------------------------------------


== Options


// component options: START
The Debezium component supports 2 options, which are listed below.



[width="100%",cols="2,5,^1,2",options="header"]
|===
| Name | Description | Default | Type
| *configuration* (consumer) | Allow pre-configured Configurations to be set, you will need to extend EmbeddedDebeziumConfiguration in order to create the configuration for the component |  | EmbeddedDebeziumConfiguration
| *basicPropertyBinding* (advanced) | Whether the component should use basic property binding (Camel 2.x) or the newer property binding with additional capabilities | false | boolean
|===
// component options: END


// endpoint options: START
The Debezium endpoint is configured using URI syntax:

----
debezium:connectorType
----

with the following path and query parameters:

=== Path Parameters (1 parameters):


[width="100%",cols="2,5,^1,2",options="header"]
|===
| Name | Description | Default | Type
| *connectorType* | *Required* The Debezium connector type that is supported by Camel Debezium component. |  | String
|===


=== Query Parameters (72 parameters):


[width="100%",cols="2,5,^1,2",options="header"]
|===
| Name | Description | Default | Type
| *bridgeErrorHandler* (consumer) | Allows for bridging the consumer to the Camel routing Error Handler, which mean any exceptions occurred while the consumer is trying to pickup incoming messages, or the likes, will now be processed as a message and handled by the routing Error Handler. By default the consumer will use the org.apache.camel.spi.ExceptionHandler to deal with exceptions, that will be logged at WARN or ERROR level and ignored. | false | boolean
| *internalKeyConverter* (consumer) | The Converter class that should be used to serialize and deserialize key data for offsets. The default is JSON converter. | org.apache.kafka.connect.json.JsonConverter | String
| *internalValueConverter* (consumer) | The Converter class that should be used to serialize and deserialize value data for offsets. The default is JSON converter. | org.apache.kafka.connect.json.JsonConverter | String
| *name* (consumer) | *Required* Unique name for the connector. Attempting to register again with the same name will fail. |  | String
| *offsetCommitPolicy* (consumer) | The name of the Java class of the commit policy. It defines when offsets commit has to be triggered based on the number of events processed and the time elapsed since the last commit. This class must implement the interface .OffsetCommitPolicy. The default is a periodic commit policy based upon time intervals. | io.debezium.embedded.spi.OffsetCommitPolicy.PeriodicCommitOffsetPolicy | String
| *offsetCommitTimeoutMs* (consumer) | Maximum number of milliseconds to wait for records to flush and partition offset data to be committed to offset storage before cancelling the process and restoring the offset data to be committed in a future attempt. The default is 5 seconds. | 5000 | long
| *offsetFlushIntervalMs* (consumer) | Interval at which to try committing offsets. The default is 1 minute. | 60000 | long
| *offsetStorage* (consumer) | The name of the Java class that is responsible for persistence of connector offsets. | org.apache.kafka.connect.storage.FileOffsetBackingStore | String
| *offsetStorageFileName* (consumer) | Path to file where offsets are to be stored. Required when offset.storage is set to the FileOffsetBackingStore |  | String
| *offsetStoragePartitions* (consumer) | The number of partitions used when creating the offset storage topic. Required when offset.storage is set to the .KafkaOffsetBackingStore. |  | int
| *offsetStorageReplication Factor* (consumer) | Replication factor used when creating the offset storage topic. Required when offset.storage is set to the KafkaOffsetBackingStore |  | int
| *offsetStorageTopic* (consumer) | The name of the Kafka topic where offsets are to be stored. Required when offset.storage is set to the KafkaOffsetBackingStore. |  | String
| *exceptionHandler* (consumer) | To let the consumer use a custom ExceptionHandler. Notice if the option bridgeErrorHandler is enabled then this option is not in use. By default the consumer will deal with exceptions, that will be logged at WARN or ERROR level and ignored. |  | ExceptionHandler
| *exchangePattern* (consumer) | Sets the exchange pattern when the consumer creates an exchange. |  | ExchangePattern
| *basicPropertyBinding* (advanced) | Whether the endpoint should use basic property binding (Camel 2.x) or the newer property binding with additional capabilities | false | boolean
| *synchronous* (advanced) | Sets whether synchronous processing should be strictly used, or Camel is allowed to use asynchronous processing (if supported). | false | boolean
| *bigintUnsignedHandlingMode* (mysql) | bigint.unsigned.handling.mode | long | String
| *binlogBufferSize* (mysql) | binlog.buffer.size | 0 | int
| *columnBlacklist* (mysql) | column.blacklist |  | String
| *connectKeepAlive* (mysql) | connect.keep.alive | true | boolean
| *connectKeepAliveIntervalMs* (mysql) | connect.keep.alive.interval.ms | 60000 | long
| *connectTimeoutMs* (mysql) | connect.timeout.ms | 30000 | int
| *databaseBlacklist* (mysql) | database.blacklist |  | String
| *databaseHistory* (mysql) | database.history | class io.debezium.relational.history.FileDatabaseHistory | String
| *databaseHistoryFileFilename* (mysql) | database.history.file.filename |  | String
| *databaseHistoryKafka BootstrapServers* (mysql) | database.history.kafka.bootstrap.servers |  | String
| *databaseHistoryKafka RecoveryAttempts* (mysql) | database.history.kafka.recovery.attempts | 100 | int
| *databaseHistoryKafka RecoveryPollIntervalMs* (mysql) | database.history.kafka.recovery.poll.interval.ms | 100 | int
| *databaseHistoryKafkaTopic* (mysql) | database.history.kafka.topic |  | String
| *databaseHistorySkip UnparseableDdl* (mysql) | database.history.skip.unparseable.ddl | false | boolean
| *databaseHistoryStoreOnly MonitoredTablesDdl* (mysql) | database.history.store.only.monitored.tables.ddl | false | boolean
| *databaseHostname* (mysql) | database.hostname |  | String
| *databaseInitialStatements* (mysql) | database.initial.statements |  | String
| *databaseJdbcDriver* (mysql) | database.jdbc.driver | class com.mysql.cj.jdbc.Driver | String
| *databasePassword* (mysql) | *Required* database.password |  | String
| *databasePort* (mysql) | database.port | 3306 | int
| *databaseServerId* (mysql) | database.server.id | 6204 | long
| *databaseServerIdOffset* (mysql) | database.server.id.offset | 10000 | long
| *databaseServerName* (mysql) | *Required* database.server.name |  | String
| *databaseSslKeystore* (mysql) | database.ssl.keystore |  | String
| *databaseSslKeystorePassword* (mysql) | database.ssl.keystore.password |  | String
| *databaseSslMode* (mysql) | database.ssl.mode | disabled | String
| *databaseSslTruststore* (mysql) | database.ssl.truststore |  | String
| *databaseSslTruststore Password* (mysql) | database.ssl.truststore.password |  | String
| *databaseUser* (mysql) | database.user |  | String
| *databaseWhitelist* (mysql) | database.whitelist |  | List
| *ddlParserMode* (mysql) | ddl.parser.mode | antlr | String
| *decimalHandlingMode* (mysql) | decimal.handling.mode | precise | String
| *enableTimeAdjuster* (mysql) | enable.time.adjuster | true | boolean
| *eventDeserializationFailure HandlingMode* (mysql) | event.deserialization.failure.handling.mode | fail | String
| *gtidNewChannelPosition* (mysql) | gtid.new.channel.position | latest | String
| *gtidSourceExcludes* (mysql) | gtid.source.excludes |  | String
| *gtidSourceFilterDmlEvents* (mysql) | gtid.source.filter.dml.events | true | boolean
| *gtidSourceIncludes* (mysql) | gtid.source.includes |  | List
| *heartbeatIntervalMs* (mysql) | heartbeat.interval.ms | 0 | int
| *heartbeatTopicsPrefix* (mysql) | heartbeat.topics.prefix | __debezium-heartbeat | String
| *includeQuery* (mysql) | include.query | false | boolean
| *includeSchemaChanges* (mysql) | include.schema.changes | true | boolean
| *inconsistentSchemaHandling Mode* (mysql) | inconsistent.schema.handling.mode | fail | String
| *maxBatchSize* (mysql) | max.batch.size | 2048 | int
| *maxQueueSize* (mysql) | max.queue.size | 8192 | int
| *pollIntervalMs* (mysql) | poll.interval.ms | 500 | long
| *snapshotDelayMs* (mysql) | snapshot.delay.ms | 0 | long
| *snapshotFetchSize* (mysql) | snapshot.fetch.size |  | int
| *snapshotLockingMode* (mysql) | snapshot.locking.mode | minimal | String
| *snapshotMode* (mysql) | snapshot.mode | initial | String
| *snapshotNewTables* (mysql) | snapshot.new.tables | off | String
| *tableBlacklist* (mysql) | table.blacklist |  | String
| *tableIgnoreBuiltin* (mysql) | table.ignore.builtin | true | boolean
| *tableWhitelist* (mysql) | table.whitelist |  | List
| *timePrecisionMode* (mysql) | time.precision.mode | adaptive_time_microseconds | String
| *tombstonesOnDelete* (mysql) | tombstones.on.delete | false | boolean
|===
// endpoint options: END
// spring-boot-auto-configure options: START
== Spring Boot Auto-Configuration

When using Spring Boot make sure to use the following Maven dependency to have support for auto configuration:

[source,xml]
----
<dependency>
  <groupId>org.apache.camel</groupId>
  <artifactId>camel-debezium-starter</artifactId>
  <version>x.x.x</version>
  <!-- use the same version as your Camel core version -->
</dependency>
----


The component supports 3 options, which are listed below.



[width="100%",cols="2,5,^1,2",options="header"]
|===
| Name | Description | Default | Type
| *camel.component.debezium.basic-property-binding* | Whether the component should use basic property binding (Camel 2.x) or the newer property binding with additional capabilities | false | Boolean
| *camel.component.debezium.configuration* | Allow pre-configured Configurations to be set, you will need to extend MySqlConnectorEmbeddedDebeziumConfiguration in order to create the configuration for the component. The option is a org.apache.camel.component.debezium.configuration.MySqlConnectorEmbeddedDebeziumConfiguration type. |  | String
| *camel.component.debezium.enabled* | Whether to enable auto configuration of the debezium component. This is enabled by default. |  | Boolean
|===
// spring-boot-auto-configure options: END

For more information about configuration:
https://debezium.io/documentation/reference/0.9/operations/embedded.html#engine-properties[https://debezium.io/documentation/reference/0.9/operations/embedded.html#engine-properties]
https://debezium.io/documentation/reference/0.9/connectors/mysql.html#connector-properties[https://debezium.io/documentation/reference/0.9/connectors/mysql.html#connector-properties]

== Message headers

=== Consumer headers

The following headers are available when consuming change events from Debezium.
[width="100%",cols="2m,2m,1m,5",options="header"]
|===
| Header constant                           | Header value                                   | Type        | Description
| DebeziumConstants.HEADER_IDENTIFIER       | "CamelDebeziumIdentifier"                      | String      | The identifier of the connector, normally is this format "{server-name}.{database-name}.{table-name}".
| DebeziumConstants.HEADER_KEY              | "CamelDebeziumKey"                             | Struct      | The key of the event, normally is the table Primary Key.
| DebeziumConstants.HEADER_SOURCE_METADATA  | "CamelDebeziumSourceMetadata"                  | Map         | The metadata about the source event, for example `table` name, database `name`, log position, etc, please refer to the Debezium documentation for more info.
| DebeziumConstants.HEADER_OPERATION        | "CamelDebeziumOperation"                       | String      | If presents, the type of event operation. Values for the connector are `c` for create (or insert), `u` for update, `d` for delete or `r` in case of a snapshot event.
| DebeziumConstants.HEADER_TIMESTAMP        | "CamelDebeziumTimestamp"                       | Long        | If presents, the time (using the system clock in the JVM) at which the connector processed the event.
| DebeziumConstants.HEADER_BEFORE           | "CamelDebeziumBefore"                          | Struct     | If presents, contains the state of the row before the event occurred.
|===

== Message body
The message body if is not `null` (in case of tombstones), it contains the state of the row after the event occurred as `Struct` format or `Map` format if you use the included Type Converter from `Struct` to `Map` (please look below for more explanation).

== Samples

=== Consuming events

Here is a very simple route that you can use in order to listen to Debezium events from MySQL connector.
[source,java]
----
from("debezium:mysql?name=dbz-test-1&offsetStorageFileName=/usr/offset-file-1.dat&databaseHostName=localhost&databaseUser=debezium&databasePassword=dbz&databaseServerName=my-app-connector&databaseHistoryFileName=/usr/history-file-1.dat")
    .log("Event received from Debezium : ${body}")
    .log("    with this identifier ${headers.CamelDebeziumIdentifier}")
    .log("    with these source metadata ${headers.CamelDebeziumSourceMetadata}")
    .log("    the event occured upon this operation '${headers.CamelDebeziumSourceOperation}'")
    .log("    on this database '${headers.CamelDebeziumSourceMetadata[db]}' and this table '${headers.CamelDebeziumSourceMetadata[table]}'")
    .log("    with the key ${headers.CamelDebeziumKey}")
    .log("    the previous value is ${headers.CamelDebeziumBefore}")
----

By default, the component will emit the events in the body and `CamelDebeziumBefore` header as https://kafka.apache.org/22/javadoc/org/apache/kafka/connect/data/Struct.html[`Struct`] data type, the reasoning behind this, is to perceive the schema information in case is needed.
However, the component as well contains a xref:manual::type-converter.adoc[Type Converter] that converts
from default output type of https://kafka.apache.org/22/javadoc/org/apache/kafka/connect/data/Struct.html[`Struct`] to `Map` in order to leverage Camel's rich xref:manual::data-format.adoc[Data Format] types which many of them work out of box with `Map` data type.
To use it, you can either add `Map.class` type when you access the message e.g: `exchange.getIn().getBody(Map.class)`, or you can convert the body always to `Map` from the route builder by adding `.convertBodyTo(Map.class)` to your Camel Route DSL after `from` statement.

We mentioned above about the schema, which can be used in case you need to perform advance data transformation and the schema is needed for that. If you choose not to convert your body to `Map`,
you can obtain the schema information as https://kafka.apache.org/22/javadoc/org/apache/kafka/connect/data/Schema.html[`Schema`] type from `Struct` like this:
[source,java]
----
from("debezium:[connectorType]?[options]])
    .process(exchange -> {
        final Struct bodyValue = exchange.getIn().getBody(Struct.class);
        final Schema schemaValue = bodyValue.schema();

        log.info("Body value is :" + bodyValue);
        log.info("With Schema : " + schemaValue);
        log.info("And fields of :" + schemaValue.fields());
        log.info("Field name has `" + schemaValue.field("name").schema() + "` type");
    });
----



*Important Note:* This component is a thin wrapper around Debezium Engine as mentioned, therefore before using this component in production, you need to understand how Debezium works and how configurations can reflect the expected behavior, especially in regards to https://debezium.io/documentation/reference/0.9/operations/embedded.html#_handling_failures[handling failures].
