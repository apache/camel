= AWS Bedrock Component
:doctitle: AWS Bedrock
:shortname: aws-bedrock
:artifactid: camel-aws-bedrock
:description: Invoke Model of AWS Bedrock service.
:since: 4.5
:supportlevel: Stable
:tabs-sync-option:
:component-header: Only producer is supported
//Manually maintained attributes
:group: AWS
:camel-spring-boot-name: aws-bedrock

*Since Camel {since}*

*{component-header}*

The AWS2 Bedrock component supports invoking a supported LLM model from
https://aws.amazon.com/bedrock/[AWS Bedrock] service.

Prerequisites

You must have a valid Amazon Web Services developer account, and be
signed up to use Amazon Bedrock. More information is available at
https://aws.amazon.com/bedrock/[Amazon Bedrock].


== URI Format

-------------------------
aws-bedrock://label[?options]
-------------------------

You can append query options to the URI in the following format:

`?options=value&option2=value&...`


// component-configure options: START

// component-configure options: END

// component options: START
include::partial$component-configure-options.adoc[]
include::partial$component-endpoint-options.adoc[]
// component options: END

// endpoint options: START

// endpoint options: END


Required Bedrock component options

You have to provide the bedrockRuntimeClient in the
Registry or your accessKey and secretKey to access
the https://aws.amazon.com/bedrock/[Amazon Bedrock] service.

== Usage

=== Static credentials, Default Credential Provider and Profile Credentials Provider

You have the possibility of avoiding the usage of explicit static credentials by specifying the useDefaultCredentialsProvider option and set it to true.

The order of evaluation for Default Credentials Provider is the following:

 - Java system properties - `aws.accessKeyId` and `aws.secretKey`.
 - Environment variables - `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.
 - Web Identity Token from AWS STS.
 - The shared credentials and config files.
 - Amazon ECS container credentials - loaded from the Amazon ECS if the environment variable `AWS_CONTAINER_CREDENTIALS_RELATIVE_URI` is set.
 - Amazon EC2 Instance profile credentials. 

You have also the possibility of using Profile Credentials Provider, by specifying the useProfileCredentialsProvider option to true and profileCredentialsName to the profile name.

Only one of static, default and profile credentials could be used at the same time.

For more information about this you can look at https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/credentials.html[AWS credentials documentation]

// component headers: START
include::partial$component-endpoint-headers.adoc[]
// component headers: END

=== Supported AWS Bedrock Models

- Titan Text Express V1 with id `amazon.titan-text-express-v1`
Express is a large language model for text generation. It is useful for a wide range of advanced, general language tasks such as open-ended text generation and conversational chat, as well as support within Retrieval Augmented Generation (RAG).

Json schema for request

[source,json]
--------------------------------------------------------------------------------
{
  "$schema": "http://json-schema.org/draft-04/schema#",
  "type": "object",
  "properties": {
    "inputText": {
      "type": "string"
    },
    "textGenerationConfig": {
      "type": "object",
      "properties": {
        "maxTokenCount": {
          "type": "integer"
        },
        "stopSequences": {
          "type": "array",
          "items": [
            {
              "type": "string"
            }
          ]
        },
        "temperature": {
          "type": "integer"
        },
        "topP": {
          "type": "integer"
        }
      },
      "required": [
        "maxTokenCount",
        "stopSequences",
        "temperature",
        "topP"
      ]
    }
  },
  "required": [
    "inputText",
    "textGenerationConfig"
  ]
}
--------------------------------------------------------------------------------

- Titan Text Lite V1 with id `amazon.titan-text-lite-v1`
Lite is a light weight efficient model, ideal for fine-tuning of English-language tasks.

Json schema for request

[source,json]
--------------------------------------------------------------------------------
{
  "$schema": "http://json-schema.org/draft-04/schema#",
  "type": "object",
  "properties": {
    "inputText": {
      "type": "string"
    },
    "textGenerationConfig": {
      "type": "object",
      "properties": {
        "maxTokenCount": {
          "type": "integer"
        },
        "stopSequences": {
          "type": "array",
          "items": [
            {
              "type": "string"
            }
          ]
        },
        "temperature": {
          "type": "integer"
        },
        "topP": {
          "type": "integer"
        }
      },
      "required": [
        "maxTokenCount",
        "stopSequences",
        "temperature",
        "topP"
      ]
    }
  },
  "required": [
    "inputText",
    "textGenerationConfig"
  ]
}
--------------------------------------------------------------------------------

- Titan Image Generator G1 with id `amazon.titan-image-generator-v1`
It generates images from text, and allows users to upload and edit an existing image. Users can edit an image with a text prompt (without a mask) or parts of an image with an image mask. You can extend the boundaries of an image with outpainting, and fill in an image with inpainting. 

Json schema for request

[source,json]
--------------------------------------------------------------------------------
{
  "$schema": "http://json-schema.org/draft-04/schema#",
  "type": "object",
  "properties": {
    "textToImageParams": {
      "type": "object",
      "properties": {
        "text": {
          "type": "string"
        },
        "negativeText": {
          "type": "string"
        }
      },
      "required": [
        "text",
        "negativeText"
      ]
    },
    "taskType": {
      "type": "string"
    },
    "imageGenerationConfig": {
      "type": "object",
      "properties": {
        "cfgScale": {
          "type": "integer"
        },
        "seed": {
          "type": "integer"
        },
        "quality": {
          "type": "string"
        },
        "width": {
          "type": "integer"
        },
        "height": {
          "type": "integer"
        },
        "numberOfImages": {
          "type": "integer"
        }
      },
      "required": [
        "cfgScale",
        "seed",
        "quality",
        "width",
        "height",
        "numberOfImages"
      ]
    }
  },
  "required": [
    "textToImageParams",
    "taskType",
    "imageGenerationConfig"
  ]
}
--------------------------------------------------------------------------------

- Titan Embeddings G1 with id `amazon.titan-embed-text-v1`
The Amazon Titan Embeddings G1 - Text – Text v1.2 can intake up to 8k tokens and outputs a vector of 1,536 dimensions. The model also works in 25+ different language

Json schema for request

[source,json]
--------------------------------------------------------------------------------
{
  "$schema": "http://json-schema.org/draft-04/schema#",
  "type": "object",
  "properties": {
    "inputText": {
      "type": "string"
    }
  },
  "required": [
    "inputText"
  ]
}
--------------------------------------------------------------------------------

- Jurassic2-Ultra with id `ai21.j2-ultra-v1`
Jurassic-2 Ultra is AI21’s most powerful model for complex tasks that require advanced text generation and comprehension.

Json schema for request

[source,json]
--------------------------------------------------------------------------------
{
  "$schema": "http://json-schema.org/draft-04/schema#",
  "type": "object",
  "properties": {
    "prompt": {
      "type": "string"
    },
    "maxTokens": {
      "type": "integer"
    },
    "temperature": {
      "type": "integer"
    },
    "topP": {
      "type": "integer"
    },
    "stopSequences": {
      "type": "array",
      "items": [
        {
          "type": "string"
        }
      ]
    },
    "presencePenalty": {
      "type": "object",
      "properties": {
        "scale": {
          "type": "integer"
        }
      },
      "required": [
        "scale"
      ]
    },
    "frequencyPenalty": {
      "type": "object",
      "properties": {
        "scale": {
          "type": "integer"
        }
      },
      "required": [
        "scale"
      ]
    }
  },
  "required": [
    "prompt",
    "maxTokens",
    "temperature",
    "topP",
    "stopSequences",
    "presencePenalty",
    "frequencyPenalty"
  ]
}
--------------------------------------------------------------------------------

- Jurassic2-Mid with id `ai21.j2-mid-v1`
Jurassic-2 Mid is less powerful than Ultra, yet carefully designed to strike the right balance between exceptional quality and affordability.

Json schema for request

[source,json]
--------------------------------------------------------------------------------
{
  "$schema": "http://json-schema.org/draft-04/schema#",
  "type": "object",
  "properties": {
    "prompt": {
      "type": "string"
    },
    "maxTokens": {
      "type": "integer"
    },
    "temperature": {
      "type": "integer"
    },
    "topP": {
      "type": "integer"
    },
    "stopSequences": {
      "type": "array",
      "items": [
        {
          "type": "string"
        }
      ]
    },
    "presencePenalty": {
      "type": "object",
      "properties": {
        "scale": {
          "type": "integer"
        }
      },
      "required": [
        "scale"
      ]
    },
    "frequencyPenalty": {
      "type": "object",
      "properties": {
        "scale": {
          "type": "integer"
        }
      },
      "required": [
        "scale"
      ]
    }
  },
  "required": [
    "prompt",
    "maxTokens",
    "temperature",
    "topP",
    "stopSequences",
    "presencePenalty",
    "frequencyPenalty"
  ]
}
--------------------------------------------------------------------------------

- Claude Instant V1.2 with id `anthropic.claude-instant-v1`
A fast, affordable yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document question-answering.

Json schema for request

[source,json]
--------------------------------------------------------------------------------
{
  "$schema": "http://json-schema.org/draft-04/schema#",
  "type": "object",
  "properties": {
    "prompt": {
      "type": "string"
    },
    "max_tokens_to_sample": {
      "type": "integer"
    },
    "stop_sequences": {
      "type": "array",
      "items": [
        {
          "type": "string"
        }
      ]
    },
    "temperature": {
      "type": "number"
    },
    "top_p": {
      "type": "integer"
    },
    "top_k": {
      "type": "integer"
    },
    "anthropic_version": {
      "type": "string"
    }
  },
  "required": [
    "prompt",
    "max_tokens_to_sample",
    "stop_sequences",
    "temperature",
    "top_p",
    "top_k",
    "anthropic_version"
  ]
}
--------------------------------------------------------------------------------

- Claude 2 with id `anthropic.claude-v2`
Anthropic's highly capable model across a wide range of tasks from sophisticated dialogue and creative content generation to detailed instruction following.

Json schema for request

[source,json]
--------------------------------------------------------------------------------
{
  "$schema": "http://json-schema.org/draft-04/schema#",
  "type": "object",
  "properties": {
    "prompt": {
      "type": "string"
    },
    "max_tokens_to_sample": {
      "type": "integer"
    },
    "stop_sequences": {
      "type": "array",
      "items": [
        {
          "type": "string"
        }
      ]
    },
    "temperature": {
      "type": "number"
    },
    "top_p": {
      "type": "integer"
    },
    "top_k": {
      "type": "integer"
    },
    "anthropic_version": {
      "type": "string"
    }
  },
  "required": [
    "prompt",
    "max_tokens_to_sample",
    "stop_sequences",
    "temperature",
    "top_p",
    "top_k",
    "anthropic_version"
  ]
}
--------------------------------------------------------------------------------

- Claude 2.1 with id `anthropic.claude-v2:1`
An update to Claude 2 that features double the context window, plus improvements across reliability, hallucination rates, and evidence-based accuracy in long document and RAG contexts.

Json schema for request

[source,json]
--------------------------------------------------------------------------------
{
  "$schema": "http://json-schema.org/draft-04/schema#",
  "type": "object",
  "properties": {
    "prompt": {
      "type": "string"
    },
    "max_tokens_to_sample": {
      "type": "integer"
    },
    "stop_sequences": {
      "type": "array",
      "items": [
        {
          "type": "string"
        }
      ]
    },
    "temperature": {
      "type": "number"
    },
    "top_p": {
      "type": "integer"
    },
    "top_k": {
      "type": "integer"
    },
    "anthropic_version": {
      "type": "string"
    }
  },
  "required": [
    "prompt",
    "max_tokens_to_sample",
    "stop_sequences",
    "temperature",
    "top_p",
    "top_k",
    "anthropic_version"
  ]
}
--------------------------------------------------------------------------------

- Claude 3 Sonnet with id `anthropic.claude-3-sonnet-20240229-v1:0`
Claude 3 Sonnet by Anthropic strikes the ideal balance between intelligence and speed—particularly for enterprise workloads.

Json schema for request

[source,json]
--------------------------------------------------------------------------------
{
  "$schema": "http://json-schema.org/draft-04/schema#",
  "type": "object",
  "properties": {
    "messages": {
      "type": "array",
      "items": [
        {
          "type": "object",
          "properties": {
            "role": {
              "type": "string"
            },
            "content": {
              "type": "array",
              "items": [
                {
                  "type": "object",
                  "properties": {
                    "type": {
                      "type": "string"
                    },
                    "text": {
                      "type": "string"
                    }
                  },
                  "required": [
                    "type",
                    "text"
                  ]
                }
              ]
            }
          },
          "required": [
            "role",
            "content"
          ]
        }
      ]
    },
    "max_tokens": {
      "type": "integer"
    },
    "anthropic_version": {
      "type": "string"
    }
  },
  "required": [
    "messages",
    "max_tokens",
    "anthropic_version"
  ]
}
--------------------------------------------------------------------------------

- Claude 3 Haiku with id `anthropic.claude-3-haiku-20240307-v1:0`
Claude 3 Haiku is Anthropic's fastest, most compact model for near-instant responsiveness. It answers simple queries and requests with speed.

Json schema for request

[source,json]
--------------------------------------------------------------------------------
{
  "$schema": "http://json-schema.org/draft-04/schema#",
  "type": "object",
  "properties": {
    "messages": {
      "type": "array",
      "items": [
        {
          "type": "object",
          "properties": {
            "role": {
              "type": "string"
            },
            "content": {
              "type": "array",
              "items": [
                {
                  "type": "object",
                  "properties": {
                    "type": {
                      "type": "string"
                    },
                    "text": {
                      "type": "string"
                    }
                  },
                  "required": [
                    "type",
                    "text"
                  ]
                }
              ]
            }
          },
          "required": [
            "role",
            "content"
          ]
        }
      ]
    },
    "max_tokens": {
      "type": "integer"
    },
    "anthropic_version": {
      "type": "string"
    }
  },
  "required": [
    "messages",
    "max_tokens",
    "anthropic_version"
  ]
}
--------------------------------------------------------------------------------

=== Bedrock Producer operations

Camel-AWS Bedrock component provides the following operation on the producer side:

- invokeTextModel
- invokeImageModel
- invokeEmbeddingsModel
- invokeTextModelStreaming
- invokeImageModelStreaming
- invokeEmbeddingsModelStreaming
- converse
- converseStream

=== Streaming Support

The streaming operations (`invokeTextModelStreaming`, `invokeImageModelStreaming`, `invokeEmbeddingsModelStreaming`) enable real-time streaming of model responses, providing lower latency and better user experience for interactive applications.

==== Streaming Output Modes

Two streaming modes are supported:

*Complete Mode (default)*: Accumulates all chunks and returns the complete response as a single message. This is the simplest mode and behaves similarly to non-streaming operations, but with lower time-to-first-token.

*Chunks Mode*: Emits each chunk as it arrives in a List. Useful for reactive streaming pipelines or when you need to process chunks individually.

==== Streaming Configuration Options

- `streamOutputMode`: Set to "complete" (default) or "chunks"
- `includeStreamingMetadata`: Whether to include metadata headers (default: true)

==== Streaming Metadata Headers

When `includeStreamingMetadata` is true, the following headers are set:

- `CamelAwsBedrockCompletionReason`: The reason the model stopped generating (e.g., "FINISH", "end_turn", "stop")
- `CamelAwsBedrockTokenCount`: The number of tokens generated (if provided by the model)
- `CamelAwsBedrockChunkCount`: The number of chunks received

==== Supported Models for Streaming

All text generation models support streaming:
- Amazon Titan (all text models)
- Anthropic Claude (all versions)
- Meta Llama (all versions)
- Mistral AI (all models)
- Cohere Command models
- Amazon Nova models

=== Converse API Support

The Converse API provides a unified, model-agnostic interface for conversational AI interactions with AWS Bedrock models. It offers several advantages over the legacy InvokeModel API:

- *Unified Interface*: Single API across all supported models (Claude, Llama, Mistral, etc.)
- *Multi-turn Conversations*: Native support for conversation history with user/assistant roles
- *Tool Use & Function Calling*: Built-in support for tools and function calling
- *System Prompts*: First-class support for system-level instructions
- *Structured Responses*: Consistent response format across all models
- *Streaming Support*: Real-time streaming with the `converseStream` operation

==== Converse Operations

Two operations are provided:

*converse*: Standard request-response conversation
[source]
--------------------------------------------------------------------------------
aws-bedrock://label?operation=converse&modelId=anthropic.claude-3-sonnet-20240229-v1:0
--------------------------------------------------------------------------------

*converseStream*: Streaming conversation with real-time chunk delivery
[source]
--------------------------------------------------------------------------------
aws-bedrock://label?operation=converseStream&modelId=anthropic.claude-3-sonnet-20240229-v1:0
--------------------------------------------------------------------------------

==== Converse Configuration Options

Converse API uses message headers for configuration:

- `CamelAwsBedrockConverseMessages` (required): List of Message objects representing the conversation
- `CamelAwsBedrockConverseSystem`: List of SystemContentBlock for system-level instructions
- `CamelAwsBedrockConverseInferenceConfig`: InferenceConfiguration for temperature, maxTokens, etc.
- `CamelAwsBedrockConverseToolConfig`: ToolConfiguration for function calling support
- `CamelAwsBedrockConverseAdditionalFields`: Document for model-specific additional fields

For streaming operations, you can also use:
- `CamelAwsBedrockStreamOutputMode`: Set to "complete" (default) or "chunks"

==== Converse Response Headers

When a conversation completes, the following headers are set:

- `CamelAwsBedrockConverseStopReason`: Why the model stopped (e.g., "end_turn", "max_tokens")
- `CamelAwsBedrockConverseUsage`: TokenUsage object with input/output token counts
- `CamelAwsBedrockConverseOutputMessage`: The complete Message object from the model
- `CamelAwsBedrockChunkCount`: (streaming only) Number of chunks received

==== Supported Models for Converse API

The Converse API supports all modern foundation models on Bedrock:

- Anthropic Claude 3 family (Haiku, Sonnet, Opus)
- Anthropic Claude 3.5 family (Sonnet v2, Haiku)
- Amazon Nova family (Micro, Lite, Pro)
- Meta Llama 3.x models
- Mistral AI models
- Cohere Command R models

NOTE: Legacy models (Claude 2.x, Claude Instant) are not supported by the Converse API. Use the `invokeTextModel` operation for those models.

== Examples

=== Producer Examples

- invokeTextModel: this operation will invoke a model from Bedrock. This is an example for both Titan Express and Titan Lite.

[source,java]
--------------------------------------------------------------------------------
from("direct:invoke")
    .to("aws-bedrock://test?bedrockRuntimeClient=#amazonBedrockRuntimeClient&operation=invokeTextModel&modelId="
                            + BedrockModels.TITAN_TEXT_EXPRESS_V1.model))
--------------------------------------------------------------------------------

and you can then send to the direct endpoint something like

[source,java]
--------------------------------------------------------------------------------
        final Exchange result = template.send("direct:invoke", exchange -> {
            ObjectMapper mapper = new ObjectMapper();
            ObjectNode rootNode = mapper.createObjectNode();
            rootNode.put("inputText",
                    "User: Generate synthetic data for daily product sales in various categories - include row number, product name, category, date of sale and price. Produce output in JSON format. Count records and ensure there are no more than 5.");

            ArrayNode stopSequences = mapper.createArrayNode();
            stopSequences.add("User:");
            ObjectNode childNode = mapper.createObjectNode();
            childNode.put("maxTokenCount", 1024);
            childNode.put("stopSequences", stopSequences);
            childNode.put("temperature", 0).put("topP", 1);

            rootNode.put("textGenerationConfig", childNode);
            exchange.getMessage().setBody(mapper.writer().writeValueAsString(rootNode));
            exchange.getMessage().setHeader(BedrockConstants.MODEL_CONTENT_TYPE, "application/json");
            exchange.getMessage().setHeader(BedrockConstants.MODEL_ACCEPT_CONTENT_TYPE, "application/json");
        });
--------------------------------------------------------------------------------

where template is a ProducerTemplate.

- invokeImageModel: this operation will invoke a model from Bedrock. This is an example for both Titan Express and Titan Lite.

[source,java]
--------------------------------------------------------------------------------
from("direct:invoke")
    .to("aws-bedrock://test?bedrockRuntimeClient=#amazonBedrockRuntimeClient&operation=invokeImageModel&modelId="
                            + BedrockModels.TITAN_IMAGE_GENERATOR_V1.model))
                        .split(body())
                        .unmarshal().base64()
                        .setHeader("CamelFileName", simple("image-${random(128)}.png")).to("file:target/generated_images")
--------------------------------------------------------------------------------

and you can then send to the direct endpoint something like

[source,java]
--------------------------------------------------------------------------------
        final Exchange result = template.send("direct:send_titan_image", exchange -> {
            ObjectMapper mapper = new ObjectMapper();
            ObjectNode rootNode = mapper.createObjectNode();
            ObjectNode textParameter = mapper.createObjectNode();
            textParameter.putIfAbsent("text",
                    new TextNode("A Sci-fi camel running in the desert"));
            rootNode.putIfAbsent("textToImageParams", textParameter);
            rootNode.putIfAbsent("taskType", new TextNode("TEXT_IMAGE"));
            ObjectNode childNode = mapper.createObjectNode();
            childNode.putIfAbsent("numberOfImages", new IntNode(3));
            childNode.putIfAbsent("quality", new TextNode("standard"));
            childNode.putIfAbsent("cfgScale", new IntNode(8));
            childNode.putIfAbsent("height", new IntNode(512));
            childNode.putIfAbsent("width", new IntNode(512));
            childNode.putIfAbsent("seed", new IntNode(0));

            rootNode.putIfAbsent("imageGenerationConfig", childNode);

            exchange.getMessage().setBody(mapper.writer().writeValueAsString(rootNode));
            exchange.getMessage().setHeader(BedrockConstants.MODEL_CONTENT_TYPE, "application/json");
            exchange.getMessage().setHeader(BedrockConstants.MODEL_ACCEPT_CONTENT_TYPE, "application/json");
        });
--------------------------------------------------------------------------------

where template is a ProducerTemplate.

- invokeEmbeddingsModel: this operation will invoke an Embeddings model from Bedrock. This is an example for Titan Embeddings G1.

[source,java]
--------------------------------------------------------------------------------
from("direct:send_titan_embeddings")
    .to("aws-bedrock:label?useDefaultCredentialsProvider=true&region=us-east-1&operation=invokeEmbeddingsModel&modelId="
    + BedrockModels.TITAN_EMBEDDINGS_G1.model)
    .to(result);
--------------------------------------------------------------------------------

and you can then send to the direct endpoint something like

[source,java]
--------------------------------------------------------------------------------
        final Exchange result = template.send("direct:send_titan_embeddings", exchange -> {
            ObjectMapper mapper = new ObjectMapper();
            ObjectNode rootNode = mapper.createObjectNode();
            rootNode.putIfAbsent("inputText",
                    new TextNode("A Sci-fi camel running in the desert"));

            exchange.getMessage().setBody(mapper.writer().writeValueAsString(rootNode));
            exchange.getMessage().setHeader(BedrockConstants.MODEL_CONTENT_TYPE, "application/json");
            exchange.getMessage().setHeader(BedrockConstants.MODEL_ACCEPT_CONTENT_TYPE, "*/*");
        });
--------------------------------------------------------------------------------

where template is a ProducerTemplate.

- invokeTextModelStreaming (Complete Mode): this operation will invoke a model from Bedrock with streaming, accumulating the complete response.

[source,java]
--------------------------------------------------------------------------------
from("direct:stream_complete")
    .to("aws-bedrock://test?useDefaultCredentialsProvider=true&region=us-east-1"
        + "&operation=invokeTextModelStreaming&modelId=" + BedrockModels.TITAN_TEXT_EXPRESS_V1.model
        + "&streamOutputMode=complete")
    .to("log:response");
--------------------------------------------------------------------------------

and you can then send to the direct endpoint something like

[source,java]
--------------------------------------------------------------------------------
        final Exchange result = template.send("direct:stream_complete", exchange -> {
            ObjectMapper mapper = new ObjectMapper();
            ObjectNode rootNode = mapper.createObjectNode();
            rootNode.put("inputText", "Write a short poem about Apache Camel.");

            ArrayNode stopSequences = mapper.createArrayNode();
            stopSequences.add("User:");
            ObjectNode childNode = mapper.createObjectNode();
            childNode.put("maxTokenCount", 512);
            childNode.put("stopSequences", stopSequences);
            childNode.put("temperature", 0).put("topP", 1);

            rootNode.put("textGenerationConfig", childNode);
            exchange.getMessage().setBody(mapper.writer().writeValueAsString(rootNode));
            exchange.getMessage().setHeader(BedrockConstants.MODEL_CONTENT_TYPE, "application/json");
            exchange.getMessage().setHeader(BedrockConstants.MODEL_ACCEPT_CONTENT_TYPE, "application/json");
        });

        // Get the complete response
        String response = result.getMessage().getBody(String.class);

        // Get streaming metadata
        Integer tokenCount = result.getMessage().getHeader(BedrockConstants.STREAMING_TOKEN_COUNT, Integer.class);
        String completionReason = result.getMessage().getHeader(BedrockConstants.STREAMING_COMPLETION_REASON, String.class);
        Integer chunkCount = result.getMessage().getHeader(BedrockConstants.STREAMING_CHUNK_COUNT, Integer.class);
--------------------------------------------------------------------------------

- invokeTextModelStreaming (Chunks Mode): this operation will invoke a model from Bedrock with streaming, emitting individual chunks.

[source,java]
--------------------------------------------------------------------------------
from("direct:stream_chunks")
    .to("aws-bedrock://test?useDefaultCredentialsProvider=true&region=us-east-1"
        + "&operation=invokeTextModelStreaming&modelId=" + BedrockModels.ANTROPHIC_CLAUDE_V35_2.model
        + "&streamOutputMode=chunks")
    .split(body())
        .to("websocket:chat-output");  // Send each chunk to websocket
--------------------------------------------------------------------------------

and you can then send to the direct endpoint something like

[source,java]
--------------------------------------------------------------------------------
        final Exchange result = template.send("direct:stream_chunks", exchange -> {
            ObjectMapper mapper = new ObjectMapper();
            ObjectNode rootNode = mapper.createObjectNode();

            ArrayNode messages = mapper.createArrayNode();
            ObjectNode message = mapper.createObjectNode();
            message.put("role", "user");

            ArrayNode content = mapper.createArrayNode();
            ObjectNode contentBlock = mapper.createObjectNode();
            contentBlock.put("type", "text");
            contentBlock.put("text", "Explain Apache Camel in one sentence.");
            content.add(contentBlock);

            message.put("content", content);
            messages.add(message);

            rootNode.put("messages", messages);
            rootNode.put("max_tokens", 200);
            rootNode.put("anthropic_version", "bedrock-2023-05-31");

            exchange.getMessage().setBody(mapper.writer().writeValueAsString(rootNode));
            exchange.getMessage().setHeader(BedrockConstants.MODEL_CONTENT_TYPE, "application/json");
            exchange.getMessage().setHeader(BedrockConstants.MODEL_ACCEPT_CONTENT_TYPE, "application/json");
        });

        // Get the list of chunks
        List<String> chunks = result.getMessage().getBody(List.class);

        // Process each chunk
        for (String chunk : chunks) {
            System.out.println("Chunk: " + chunk);
        }
--------------------------------------------------------------------------------

- converse: this operation uses the unified Converse API for model-agnostic conversations.

[source,java]
--------------------------------------------------------------------------------
from("direct:converse")
    .to("aws-bedrock://test?useDefaultCredentialsProvider=true&region=us-east-1"
        + "&operation=converse&modelId=" + BedrockModels.ANTROPHIC_CLAUDE_V3.model)
    .to("log:response");
--------------------------------------------------------------------------------

and you can then send to the direct endpoint something like

[source,java]
--------------------------------------------------------------------------------
        final Exchange result = template.send("direct:converse", exchange -> {
            // Create a conversation message
            List<software.amazon.awssdk.services.bedrockruntime.model.Message> messages = new ArrayList<>();
            messages.add(software.amazon.awssdk.services.bedrockruntime.model.Message.builder()
                    .role(software.amazon.awssdk.services.bedrockruntime.model.ConversationRole.USER)
                    .content(software.amazon.awssdk.services.bedrockruntime.model.ContentBlock
                            .fromText("What is Apache Camel and what are its main features?"))
                    .build());

            exchange.getMessage().setHeader(BedrockConstants.CONVERSE_MESSAGES, messages);

            // Optional: Add inference configuration
            software.amazon.awssdk.services.bedrockruntime.model.InferenceConfiguration inferenceConfig
                    = software.amazon.awssdk.services.bedrockruntime.model.InferenceConfiguration.builder()
                            .maxTokens(500)
                            .temperature(0.7f)
                            .build();
            exchange.getMessage().setHeader(BedrockConstants.CONVERSE_INFERENCE_CONFIG, inferenceConfig);

            // Optional: Add system prompt
            List<software.amazon.awssdk.services.bedrockruntime.model.SystemContentBlock> systemPrompt = new ArrayList<>();
            systemPrompt.add(software.amazon.awssdk.services.bedrockruntime.model.SystemContentBlock
                    .fromText("You are a helpful assistant that explains software concepts clearly and concisely."));
            exchange.getMessage().setHeader(BedrockConstants.CONVERSE_SYSTEM, systemPrompt);
        });

        // Get the response text
        String response = result.getMessage().getBody(String.class);

        // Get metadata from headers
        String stopReason = result.getMessage().getHeader(BedrockConstants.CONVERSE_STOP_REASON, String.class);
        software.amazon.awssdk.services.bedrockruntime.model.TokenUsage usage
                = result.getMessage().getHeader(BedrockConstants.CONVERSE_USAGE,
                        software.amazon.awssdk.services.bedrockruntime.model.TokenUsage.class);

        System.out.println("Response: " + response);
        System.out.println("Stop reason: " + stopReason);
        System.out.println("Input tokens: " + usage.inputTokens());
        System.out.println("Output tokens: " + usage.outputTokens());
--------------------------------------------------------------------------------

- converseStream (Complete Mode): this operation uses the Converse API with streaming, accumulating the complete response.

[source,java]
--------------------------------------------------------------------------------
from("direct:converse_stream")
    .to("aws-bedrock://test?useDefaultCredentialsProvider=true&region=us-east-1"
        + "&operation=converseStream&modelId=" + BedrockModels.ANTROPHIC_CLAUDE_V3.model)
    .to("log:response");
--------------------------------------------------------------------------------

and you can then send to the direct endpoint something like

[source,java]
--------------------------------------------------------------------------------
        final Exchange result = template.send("direct:converse_stream", exchange -> {
            // Create a conversation message
            List<software.amazon.awssdk.services.bedrockruntime.model.Message> messages = new ArrayList<>();
            messages.add(software.amazon.awssdk.services.bedrockruntime.model.Message.builder()
                    .role(software.amazon.awssdk.services.bedrockruntime.model.ConversationRole.USER)
                    .content(software.amazon.awssdk.services.bedrockruntime.model.ContentBlock
                            .fromText("Explain the Enterprise Integration Patterns in three sentences."))
                    .build());

            exchange.getMessage().setHeader(BedrockConstants.CONVERSE_MESSAGES, messages);
            exchange.getMessage().setHeader(BedrockConstants.STREAM_OUTPUT_MODE, "complete");

            // Optional: Add inference configuration
            software.amazon.awssdk.services.bedrockruntime.model.InferenceConfiguration inferenceConfig
                    = software.amazon.awssdk.services.bedrockruntime.model.InferenceConfiguration.builder()
                            .maxTokens(300)
                            .temperature(0.5f)
                            .build();
            exchange.getMessage().setHeader(BedrockConstants.CONVERSE_INFERENCE_CONFIG, inferenceConfig);
        });

        // Get the complete streamed response
        String response = result.getMessage().getBody(String.class);
        Integer chunkCount = result.getMessage().getHeader(BedrockConstants.STREAMING_CHUNK_COUNT, Integer.class);

        System.out.println("Response: " + response);
        System.out.println("Received " + chunkCount + " chunks");
--------------------------------------------------------------------------------

- converseStream (Chunks Mode): this operation uses the Converse API with streaming, emitting individual chunks.

[source,java]
--------------------------------------------------------------------------------
from("direct:converse_stream_chunks")
    .to("aws-bedrock://test?useDefaultCredentialsProvider=true&region=us-east-1"
        + "&operation=converseStream&modelId=" + BedrockModels.ANTROPHIC_CLAUDE_V3.model)
    .split(body())
        .to("websocket:chat-output");  // Send each chunk to websocket
--------------------------------------------------------------------------------

and you can then send to the direct endpoint something like

[source,java]
--------------------------------------------------------------------------------
        final Exchange result = template.send("direct:converse_stream_chunks", exchange -> {
            // Create a conversation message
            List<software.amazon.awssdk.services.bedrockruntime.model.Message> messages = new ArrayList<>();
            messages.add(software.amazon.awssdk.services.bedrockruntime.model.Message.builder()
                    .role(software.amazon.awssdk.services.bedrockruntime.model.ConversationRole.USER)
                    .content(software.amazon.awssdk.services.bedrockruntime.model.ContentBlock
                            .fromText("Write a haiku about software integration."))
                    .build());

            exchange.getMessage().setHeader(BedrockConstants.CONVERSE_MESSAGES, messages);
            exchange.getMessage().setHeader(BedrockConstants.STREAM_OUTPUT_MODE, "chunks");
        });

        // Get the list of chunks
        List<String> chunks = result.getMessage().getBody(List.class);

        // Process each chunk as it was received
        for (String chunk : chunks) {
            System.out.println("Chunk: " + chunk);
        }
--------------------------------------------------------------------------------

- Multi-turn Conversation with Converse API: demonstrates maintaining conversation history.

[source,java]
--------------------------------------------------------------------------------
from("direct:conversation")
    .to("aws-bedrock://test?useDefaultCredentialsProvider=true&region=us-east-1"
        + "&operation=converse&modelId=" + BedrockModels.ANTROPHIC_CLAUDE_V3.model)
    .to("log:response");
--------------------------------------------------------------------------------

and you can then send to the direct endpoint something like

[source,java]
--------------------------------------------------------------------------------
        // Maintain conversation history
        List<software.amazon.awssdk.services.bedrockruntime.model.Message> conversationHistory = new ArrayList<>();

        // First turn
        conversationHistory.add(software.amazon.awssdk.services.bedrockruntime.model.Message.builder()
                .role(software.amazon.awssdk.services.bedrockruntime.model.ConversationRole.USER)
                .content(software.amazon.awssdk.services.bedrockruntime.model.ContentBlock
                        .fromText("What is Apache Camel?"))
                .build());

        Exchange result1 = template.send("direct:conversation", exchange -> {
            exchange.getMessage().setHeader(BedrockConstants.CONVERSE_MESSAGES,
                    new ArrayList<>(conversationHistory));
        });

        // Add assistant's response to history
        software.amazon.awssdk.services.bedrockruntime.model.Message assistantMessage
                = result1.getMessage().getHeader(BedrockConstants.CONVERSE_OUTPUT_MESSAGE,
                        software.amazon.awssdk.services.bedrockruntime.model.Message.class);
        conversationHistory.add(assistantMessage);

        // Second turn - follow-up question
        conversationHistory.add(software.amazon.awssdk.services.bedrockruntime.model.Message.builder()
                .role(software.amazon.awssdk.services.bedrockruntime.model.ConversationRole.USER)
                .content(software.amazon.awssdk.services.bedrockruntime.model.ContentBlock
                        .fromText("Can you give me a simple example?"))
                .build());

        Exchange result2 = template.send("direct:conversation", exchange -> {
            exchange.getMessage().setHeader(BedrockConstants.CONVERSE_MESSAGES,
                    new ArrayList<>(conversationHistory));
        });

        String followUpResponse = result2.getMessage().getBody(String.class);
        System.out.println("Follow-up response: " + followUpResponse);
--------------------------------------------------------------------------------

== Dependencies

Maven users will need to add the following dependency to their pom.xml.

*pom.xml*

[source,xml]
---------------------------------------
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-aws-bedrock</artifactId>
    <version>${camel-version}</version>
</dependency>
---------------------------------------

where `$\{camel-version}` must be replaced by the actual version of Camel.


include::spring-boot:partial$starter.adoc[]
