= OpenAI Component
:doctitle: OpenAI
:shortname: openai
:artifactid: camel-openai
:description: OpenAI endpoint for chat completion and embeddings.
:since: 4.17
:supportlevel: Preview
:tabs-sync-option:
:component-header: Only producer is supported
//Manually maintained attributes
:group: AI
:camel-spring-boot-name: openai

*Since Camel {since}*

*{component-header}*

The OpenAI component provides integration with OpenAI and OpenAI-compatible APIs for chat completion and text embeddings using the official openai-java SDK.

Maven users will need to add the following dependency to their `pom.xml` for this component:

[source,xml]
----
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-openai</artifactId>
    <version>x.x.x</version>
    <!-- use the same version as your Camel core version -->
</dependency>
----

== URI Format

[source]
----
openai:operation[?options]
----

Supported operations:

* `chat-completion` - Generate chat completions using language models
* `embeddings` - Generate vector embeddings from text for semantic search and RAG applications

// component options: START
include::partial$component-configure-options.adoc[]
include::partial$component-endpoint-options.adoc[]
include::partial$component-endpoint-headers.adoc[]
// component options: END

== Usage

=== Authentication

Set `baseUrl` to your providers endpoint (default: `https://api.openai.com/v1`).

API key resolution order:

- Endpoint `apiKey`
- Component `apiKey`
- Environment variable `OPENAI_API_KEY`
- System property `openai.api.key`

[NOTE]
====
The API key can be omitted if using OpenAI-compatible providers that don't require authentication (e.g., some local LLM servers).
====

=== Basic Chat Completion with String Input

[tabs]
====
Java::
+
[source,java]
----
from("direct:chat")
    .setBody(constant("What is Apache Camel?"))
    .to("openai:chat-completion")
    .log("Response: ${body}");
----

YAML::
+
[source,yaml]
----
- route:
    from:
      uri: direct:chat
      steps:
        - to:
            uri: openai:chat-completion
            parameters:
              userMessage: What is Apache Camel?
        - log: "Response: ${body}"
----
====

=== File-Backed Prompt with Text File

.Usage example:
[source,java]
----
from("file:prompts?noop=true")
    .to("openai:chat-completion")
    .log("Response: ${body}");
----



=== Image File Input with Vision Model

.Usage example:
[source,java]
----
from("file:images?noop=true")
    .to("openai:chat-completion?model=gpt-4.1-mini?userMessage=Describe what you see in this image")
    .log("Response: ${body}");
----

[NOTE]
====
When using image files, the userMessage is required. Supported image formats are detected by MIME type (e.g., `image/png`, `image/jpeg`, `image/gif`, `image/webp`).
====

=== Streaming Response

When `streaming=true`, the component returns an `Iterator<ChatCompletionChunk>` in the message body. You can consume this iterator using Camel's streaming EIPs or process it directly:

.Usage example:
[source,yaml]
----
- route:
    id: route-1145
    from:
      id: from-1972
      uri: timer
      parameters:
        repeatCount: 1
        timerName: timer
      steps:
        - to:
            id: to-1301
            uri: openai:chat-completion
            parameters:
              userMessage: In one sentence, what is Apache Camel?
              streaming: true
        - split:
            id: split-3196
            steps:
              - marshal:
                  id: marshal-3773
                  json:
                    library: Jackson
              - log:
                  id: log-6722
                  message: ${body}
            simple:
              expression: ${body}
            streaming: true
----

=== Structured Output with outputClass

.When `outputClass` is set, the model is instructed to produce JSON matching the given class, but the component returns the raw String. Deserialize the body yourself (e.g., with Camel's Jackson) if you need a typed object.

.Usage example:
[source,java]
----
public class Person {
    public String name;
    public int age;
    public String occupation;
}

from("direct:structured")
    .setBody(constant("Generate a person profile for a software engineer"))
    .to("openai:chat-completion?baseUrl=https://api.openai.com/v1&outputClass=com.example.Person")
    .log("Structured response: ${body}");
----

=== Structured Output with JSON Schema

The `jsonSchema` option instructs the model to return JSON that conforms to the provided schema. The response will be valid JSON but is not automatically validated against the schema:

.Usage example:
[source,java]
----
from("direct:json-schema")
    .setBody(constant("Create a product description"))
    .setHeader("CamelOpenAIJsonSchema", constant("{\"type\":\"object\",\"properties\":{\"name\":{\"type\":\"string\"},\"price\":{\"type\":\"number\"}}}"))
    .to("openai:chat-completion")
    .log("JSON response: ${body}");
----

You can also load the schema from a resource file:

.Usage example:
[source,java]
----
from("direct:json-schema-resource")
    .setBody(constant("Create a product description"))
    .to("openai:chat-completion?jsonSchema=resource:classpath:schemas/product.schema.json")
    .log("JSON response: ${body}");
----

[NOTE]
====
For full schema validation, integrate with the `camel-json-validator` component after receiving the response.
====

=== Conversation Memory (Per Exchange)

.Usage example:
[source,java]
----
from("direct:conversation")
    .setBody(constant("My name is Alice"))
    .to("openai:chat-completion?conversationMemory=true")
    .log("First response: ${body}")
    .setBody(constant("What is my name?"))
    .to("openai:chat-completion?conversationMemory=true")
    .log("Second response: ${body}"); // Will remember "Alice"
----

=== Using Third-Party or Local OpenAI-Compatible Endpoint

.Usage example:
[source,java]
----
from("direct:local")
    .setBody(constant("Hello from local LLM"))
    .to("openai:chat-completion?baseUrl=http://localhost:1234/v1&model=local-model")
    .log("${body}");
----

== Input Handling

The component accepts the following types of input in the message body:

1. *String*: The prompt text is taken directly from the body
2. *File*: Used for file-based prompts. The component handles two types of files:
   * *Text files* (MIME type starting with `text/`): The file content is read and used as the prompt. If userMessage endpoint option or `CamelOpenAIUserMessage` is set, it overrides the file content
   * *Image files* (MIME type starting with `image/`): The file is encoded as a base64 data URL and sent to vision-capable models. The userMessage is **required** when using image files

[NOTE]
====
When using `File` input, the component uses `Files.probeContentType()` to detect the file type. Ensure your system has proper MIME type detection configured.
====

== Output Handling

=== Default Mode
The full model response is returned as a String in the message body.

=== Streaming Mode
When `streaming=true`, the message body contains an `Iterator<ChatCompletionChunk>` suitable for Camel streaming EIPs (such as `split()` with `streaming()`).

IMPORTANT:
* Resource cleanup is handled automatically when the Exchange completes (success or failure)
* Conversation memory is **not** automatically updated for streaming responses (only for non-streaming responses)

=== Structured Outputs

==== Using outputClass
The model is instructed to return JSON matching the specified class, but the response body remains a String.

==== Using jsonSchema
The `jsonSchema` option instructs the model to return JSON conforming to the provided schema. The response will be valid JSON but is not automatically validated against the schema. For full schema validation, integrate with the `camel-json-validator` component after receiving the response.

The JSON schema must be a valid JSON object. Invalid schema strings will result in an `IllegalArgumentException`.

== Conversation Memory

When `conversationMemory=true`, the component maintains conversation history in the `CamelOpenAIConversationHistory` exchange property (configurable via `conversationHistoryProperty` option). This history is scoped to a single Exchange and allows multi-turn conversations within a route.

IMPORTANT:
* Conversation history is automatically updated with each assistant response for **non-streaming** responses only
* The history is stored as a `List<ChatCompletionMessageParam>` in the Exchange property
* The history persists across multiple calls to the endpoint within the same Exchange
* You can manually set the `CamelOpenAIConversationHistory` exchange property to provide custom conversation context

Example of manual conversation history:

.Usage example:
[source,java]
----
List<ChatCompletionMessageParam> history = new ArrayList<>();
history.add(ChatCompletionMessageParam.ofUser(/* ... */));
history.add(ChatCompletionMessageParam.ofAssistant(/* ... */));

from("direct:with-history")
    .setBody(constant("Continue the conversation"))
    .setProperty("CamelOpenAIConversationHistory", constant(history))
    .to("openai:chat-completion?conversationMemory=true")
    .log("${body}");
----

== Compatibility

This component works with any OpenAI API-compatible endpoint by setting the `baseUrl` parameter. This includes:

- OpenAI official API (`https://api.openai.com/v1`)
- Azure OpenAI (may require additional configuration)
- Local LLM servers (e.g., Ollama, LM Studio, LocalAI)
- Third-party OpenAI-compatible providers

[NOTE]
====
When using local or third-party providers, ensure they support the chat completions and/or embeddings API endpoint format. Some providers may have different authentication requirements or API variations.
====

=== Embedding Models by Provider

[cols="1,2,1"]
|===
| Provider | Recommended Model | Dimensions

| OpenAI | `text-embedding-3-small` | 1536 (reducible to 256, 512, 1024)
| OpenAI | `text-embedding-3-large` | 3072 (reducible)
| Ollama | `nomic-embed-text` | 768
| Ollama | `mxbai-embed-large` | 1024
| Mistral | `mistral-embed` | 1024
| Azure OpenAI | `text-embedding-ada-002` | 1536
|===

.Example using Ollama for local embeddings:
[source,yaml]
----
- to:
    uri: openai:embeddings
    parameters:
      baseUrl: http://localhost:11434/v1
      embeddingModel: nomic-embed-text
----

== Embeddings Operation

The `embeddings` operation generates vector embeddings from text, which can be used for semantic search, similarity comparison, and RAG (Retrieval-Augmented Generation) applications.

=== Basic Embedding

[tabs]
====
Java::
+
[source,java]
----
from("direct:embed")
    .setBody(constant("What is Apache Camel?"))
    .to("openai:embeddings?embeddingModel=nomic-embed-text")
----

YAML::
+
[source,yaml]
----
- route:
    from:
      uri: direct:embed
      steps:
        - to:
            uri: openai:embeddings
            parameters:
              embeddingModel: nomic-embed-text
----
====

The response body is the embedding vector data:

* Single input: `List<Float>` (a single embedding vector)
* Batch input: `List<List<Float>>` (one embedding vector per input string)

Additional metadata (model, token usage, vector size, count) is exposed via headers (see `OpenAIConstants`).

=== Batch Embedding

You can embed multiple texts in a single request by passing a `List<String>`:

[source,java]
----
from("direct:batch-embed")
    .setBody(constant(List.of("First text", "Second text", "Third text")))
    .to("openai:embeddings?embeddingModel=nomic-embed-text")
    .log("Generated ${header.CamelOpenAIEmbeddingCount} embeddings");
----

=== Direct Vector Database Integration

For single-input requests, the component returns a raw `List<Float>` embedding vector, enabling direct chaining to vector database components.

==== PostgreSQL + pgvector (Recommended)

[source,yaml]
----
# Index documents in PostgreSQL with pgvector
- route:
    from:
      uri: direct:index
      steps:
        - setVariable:
            name: text
            simple: "${body}"
        - to:
            uri: openai:embeddings
            parameters:
              embeddingModel: nomic-embed-text
        - setVariable:
            name: embedding
            simple: "${body.toString()}"
        - to:
            uri: sql:INSERT INTO documents (content, embedding) VALUES (:#text, :#embedding::vector)
----

==== Alternative: Dedicated Vector Databases

For specialized vector workloads, you can also use `camel-qdrant`, `camel-weaviate`, `camel-milvus`, or `camel-pinecone`:

=== Similarity Calculation

The component can automatically calculate cosine similarity when a reference embedding is provided:

[source,java]
----
List<Float> referenceEmbedding = /* previously computed embedding */;

from("direct:compare")
    .setBody(constant("New text to compare"))
    .setHeader("CamelOpenAIReferenceEmbedding", constant(referenceEmbedding))
    .to("openai:embeddings?embeddingModel=nomic-embed-text")
    .log("Similarity score: ${header.CamelOpenAISimilarityScore}");
----

You can also use `SimilarityUtils` directly for manual calculations:

[source,java]
----
import org.apache.camel.component.openai.SimilarityUtils;

double similarity = SimilarityUtils.cosineSimilarity(embedding1, embedding2);
double distance = SimilarityUtils.euclideanDistance(embedding1, embedding2);
List<Float> normalized = SimilarityUtils.normalize(embedding);
----

=== Embeddings Output Headers

The following headers are set after an embeddings request:

[cols="1,1,3"]
|===
| Header | Type | Description

| `CamelOpenAIEmbeddingResponseModel` | String | The model used for embedding
| `CamelOpenAIEmbeddingCount` | Integer | Number of embeddings returned
| `CamelOpenAIEmbeddingVectorSize` | Integer | Dimension of each embedding vector
| `CamelOpenAIPromptTokens` | Integer | Tokens used in the input
| `CamelOpenAITotalTokens` | Integer | Total tokens used
| `CamelOpenAIOriginalText` | String/List | Original input text(s)
| `CamelOpenAISimilarityScore` | Double | Cosine similarity (if reference embedding provided)
|===

== Error Handling

The component may throw the following exceptions:

* `IllegalArgumentException`:
  ** When an invalid operation is specified (supported: `chat-completion`, `embeddings`)
  ** When message body or user message is missing
  ** When image file is provided without userMessage (chat-completion)
  ** When unsupported file type is provided (only text and image files are supported)
  ** When invalid JSON schema string is provided
* API-specific exceptions from the OpenAI SDK for network errors, authentication failures, rate limiting, etc.

include::spring-boot:partial$starter.adoc[]
