= OpenAI Component
:doctitle: OpenAI
:shortname: openai
:artifactid: camel-openai
:description: OpenAI endpoint for chat completion.
:since: 4.16
:supportlevel: Preview
:tabs-sync-option:
:component-header: Only producer is supported
//Manually maintained attributes
:group: AI

*Since Camel {since}*

*{component-header}*

The OpenAI component provides integration with OpenAI and OpenAI-compatible APIs for chat completion using the official openai-java SDK.

Maven users will need to add the following dependency to their `pom.xml`
for this component:

[source,xml]
----
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-openai</artifactId>
    <version>x.x.x</version>
    <!-- use the same version as your Camel core version -->
</dependency>
----

== URI Format

[source]
----
openai:operation[?options]
----

Currently, only the `chat-completion` operation is supported.

// component-configure options: START

// component-configure options: END

// component options: START
include::partial$component-configure-options.adoc[]
include::partial$component-endpoint-options.adoc[]
// component options: END

// endpoint options: START

// endpoint options: END

// component headers: START
include::partial$component-endpoint-headers.adoc[]
// component headers: END

== Usage

=== Authentication and Configuration

The component reads credentials and configuration from multiple sources in the following priority order:

1. URI parameters (highest priority)
2. Component-level configuration
3. Environment variables (`OPENAI_API_KEY`)
4. System properties / application.properties (`openai.api.key`)

[NOTE]
====
The API key can be omitted if using OpenAI-compatible providers that don't require authentication (e.g., some local LLM servers).
====

=== Basic Chat Completion with String Input

.Usage example:
[source,java]
----
from("direct:chat")
    .setBody(constant("What is Apache Camel?"))
    .to("openai:chat-completion?model=gpt-5")
    .log("Response: ${body}");
----

=== Using Configuration from Environment

.Usage example:
[source,java]
----
// Assumes OPENAI_API_KEY is set in environment
from("timer:question?repeatCount=1")
    .setBody(constant("Explain microservices in one sentence"))
    .to("openai:chat-completion")
    .log("${body}");
----

=== File-Backed Prompt with Text File

.Usage example:
[source,java]
----
from("file:prompts?noop=true")
    .to("openai:chat-completion?model=gpt-5")
    .log("Response: ${body}");
----

The component automatically reads text files (detected by MIME type starting with `text/`) and uses the file content as the prompt. Alternatively, you can provide a user prompt header to combine with the file content:

.Usage example:
[source,java]
----
from("file:prompts?noop=true")
    .setHeader("CamelOpenAIUserPrompt", constant("Summarize this document:"))
    .to("openai:chat-completion?model=gpt-5")
    .log("Response: ${body}");
----

=== Image File Input with Vision Model

.Usage example:
[source,java]
----
from("file:images?noop=true")
    .setHeader("CamelOpenAIUserPrompt", constant("Describe what you see in this image"))
    .to("openai:chat-completion?model=gpt-4.1-mini")
    .log("Response: ${body}");
----

[NOTE]
====
When using image files, the `CamelOpenAIUserPrompt` header is required. Supported image formats are detected by MIME type (e.g., `image/png`, `image/jpeg`, `image/gif`, `image/webp`).
====

=== Streaming Response

When `streaming=true`, the component returns an `Iterator<ChatCompletionChunk>` in the message body. You can consume this iterator using Camel's streaming EIPs or process it directly:

.Usage example:
[source,yaml]
----
- route:
    id: route-1145
    from:
      id: from-1972
      uri: timer
      parameters:
        repeatCount: 1
        timerName: timer
      steps:
        - setBody:
            constant: In one sentence, what is Apache Camel?
        - to: openai:chat-completion?model=gpt-5&streaming=true
        - split:
            id: split-3196
            steps:
              - marshal:
                  id: marshal-3773
                  json:
                    library: Jackson
              - log:
                  id: log-6722
                  message: ${body}
            simple:
              expression: ${body}
            streaming: true
----

=== Structured Output with outputClass

.Usage example:
[source,java]
----
public class Person {
    public String name;
    public int age;
    public String occupation;
}

from("direct:structured")
    .setBody(constant("Generate a person profile for a software engineer"))
    .to("openai:chat-completion?outputClass=com.example.Person&model=gpt-5")
    .log("Structured response: ${body}");
----

=== Structured Output with JSON Schema

The `jsonSchema` option instructs the model to return JSON that conforms to the provided schema. The response will be valid JSON but is not automatically validated against the schema:

.Usage example:
[source,java]
----
from("direct:json-schema")
    .setBody(constant("Create a product description"))
    .setHeader("CamelOpenAIJsonSchema", constant("{\"type\":\"object\",\"properties\":{\"name\":{\"type\":\"string\"},\"price\":{\"type\":\"number\"}}}"))
    .to("openai:chat-completion?model=gpt-5")
    .log("JSON response: ${body}");
----

You can also load the schema from a resource file:

.Usage example:
[source,java]
----
from("direct:json-schema-resource")
    .setBody(constant("Create a product description"))
    .setHeader("CamelOpenAIJsonSchema")
        .simple("resource:classpath:schemas/product.schema.json")
    .to("openai:chat-completion?model=gpt-5")
    .log("JSON response: ${body}");
----

[NOTE]
====
For full schema validation, integrate with the `camel-json-validator` component after receiving the response.
====

=== Conversation Memory (Per Exchange)

.Usage example:
[source,java]
----
from("direct:conversation")
    .setBody(constant("My name is Alice"))
    .to("openai:chat-completion?conversationMemory=true")
    .log("First response: ${body}")
    .setBody(constant("What is my name?"))
    .to("openai:chat-completion?conversationMemory=true")
    .log("Second response: ${body}"); // Will remember "Alice"
----

=== Using Third-Party or Local OpenAI-Compatible Endpoint

.Usage example:
[source,java]
----
from("direct:local")
    .setBody(constant("Hello from local LLM"))
    .to("openai:chat-completion?baseUrl=http://localhost:1234/v1&model=local-model")
    .log("${body}");
----

=== Dynamic Parameters via Headers

.Usage example:
[source,java]
----
from("direct:dynamic")
    .setBody(constant("Write a haiku"))
    .setHeader("CamelOpenAIModel", constant("gpt-5"))
    .setHeader("CamelOpenAITemperature", constant(0.7))
    .setHeader("CamelOpenAIMaxTokens", constant(100))
    .to("openai:chat-completion")
    .log("${body}");
----

== Input Handling

The component accepts the following types of input in the message body:

1. *String*: The prompt text is taken directly from the body
2. *File*: Used for file-based prompts. The component handles two types of files:
   * *Text files* (MIME type starting with `text/`): The file content is read and used as the prompt. Optionally, you can set `CamelOpenAIUserPrompt` to combine a prompt with the file content
   * *Image files* (MIME type starting with `image/`): The file is encoded as a base64 data URL and sent to vision-capable models. The `CamelOpenAIUserPrompt` header is **required** when using image files

[NOTE]
====
When using `File` input, the component uses `Files.probeContentType()` to detect the file type. Ensure your system has proper MIME type detection configured.
====

== Output Handling

=== Default Mode
The full model response is returned as a String in the message body.

=== Streaming Mode
When `streaming=true`, the message body contains an `Iterator<ChatCompletionChunk>` suitable for Camel streaming EIPs (such as `split()` with `streaming()`). 

IMPORTANT: 
* Resource cleanup is handled automatically when the Exchange completes (success or failure)
* Conversation memory is **not** automatically updated for streaming responses (only for non-streaming responses)

=== Structured Outputs

==== Using outputClass
The response body is automatically deserialized to an instance of the specified class.

==== Using jsonSchema
The `jsonSchema` option instructs the model to return JSON conforming to the provided schema. The response will be valid JSON but is not automatically validated against the schema. For full schema validation, integrate with the `camel-json-validator` component after receiving the response.

The JSON schema must be a valid JSON object. Invalid schema strings will result in an `IllegalArgumentException`.

== Conversation Memory

When `conversationMemory=true`, the component maintains conversation history in the `CamelOpenAIConversationHistory` header (configurable via `conversationHistoryHeader` option). This history is scoped to a single Exchange and allows multi-turn conversations within a route.

IMPORTANT:
* Conversation history is automatically updated with each assistant response for **non-streaming** responses only
* The history is stored as a `List<ChatCompletionMessageParam>` in the Exchange header
* The history persists across multiple calls to the endpoint within the same Exchange
* You can manually set the `CamelOpenAIConversationHistory` header to provide custom conversation context

Example of manual conversation history:

.Usage example:
[source,java]
----
List<ChatCompletionMessageParam> history = new ArrayList<>();
history.add(ChatCompletionMessageParam.ofUser(/* ... */));
history.add(ChatCompletionMessageParam.ofAssistant(/* ... */));

from("direct:with-history")
    .setBody(constant("Continue the conversation"))
    .setHeader("CamelOpenAIConversationHistory", constant(history))
    .to("openai:chat-completion?conversationMemory=true")
    .log("${body}");
----

== Compatibility

This component works with any OpenAI API-compatible endpoint by setting the `baseUrl` parameter. This includes:

- OpenAI official API (default: `https://api.openai.com/v1`)
- Azure OpenAI (may require additional configuration)
- Local LLM servers (e.g., Ollama, LM Studio, LocalAI)
- Third-party OpenAI-compatible providers

[NOTE]
====
When using local or third-party providers, ensure they support the chat completions API endpoint format. Some providers may have different authentication requirements or API variations.
====

== Error Handling

The component may throw the following exceptions:

* `IllegalArgumentException`: 
  ** When an invalid operation is specified (only `chat-completion` is supported)
  ** When message body or user prompt is missing
  ** When image file is provided without `CamelOpenAIUserPrompt` header
  ** When unsupported file type is provided (only text and image files are supported)
  ** When invalid JSON schema string is provided
* `IllegalStateException`: When the endpoint or client cannot be initialized
* API-specific exceptions from the OpenAI SDK for network errors, authentication failures, rate limiting, etc.
