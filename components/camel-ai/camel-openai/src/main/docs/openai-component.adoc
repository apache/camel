= OpenAI Component
:doctitle: OpenAI
:shortname: openai
:artifactid: camel-openai
:description: OpenAI endpoint for chat completion.
:since: 4.17
:supportlevel: Preview
:tabs-sync-option:
:component-header: Only producer is supported

*Since Camel {since}*

*{component-header}*

The OpenAI component provides integration with OpenAI and OpenAI-compatible APIs for chat completion using the official openai-java SDK.

== URI Format

[source]
----
openai:operation[?options]
----

Currently, only the `chat-completion` operation is supported.

== Component Options

The component supports the following options:

[width="100%",cols="2,5,^1,2",options="header"]
|===
| Name | Description | Default | Type
| *apiKey* | Default API key for all endpoints. Can also be set via OPENAI_API_KEY environment variable or application.properties. | | String
| *baseUrl* | Default base URL for OpenAI API. Use this to connect to local or third-party OpenAI-compatible providers. | | String
| *lazyStartProducer* | Whether the producer should be started lazy (on the first message). By starting lazy you can use this to allow CamelContext and routes to startup in situations where a producer may otherwise fail during starting and cause the route to fail being started. | false | boolean
| *autowiredEnabled* | Whether autowiring is enabled. This is used for automatic autowiring options (the option must be marked as autowired) by looking up in the registry to find if there is a single instance of matching type, which then gets configured on the component. | true | boolean
|===

== Endpoint Options

[width="100%",cols="2,5,^1,2",options="header"]
|===
| Name | Description | Default | Type
| *apiKey* | OpenAI API key. Can also be set via OPENAI_API_KEY environment variable or application.properties. | | String
| *baseUrl* | Base URL for OpenAI API. Defaults to OpenAI's official endpoint. Can be used for local or third-party providers. | | String
| *model* | The model to use for chat completion | gpt-3.5-turbo | String
| *temperature* | Temperature for response generation (0.0 to 2.0) | 1.0 | Double
| *topP* | Top P for response generation (0.0 to 1.0) | | Double
| *maxTokens* | Maximum number of tokens to generate | | Integer
| *streaming* | Enable streaming responses | false | boolean
| *outputClass* | Fully qualified class name for structured output using response format | | String
| *jsonSchema* | JSON schema for structured output validation | | String
| *conversationMemory* | Enable conversation memory per Exchange | false | boolean
| *conversationHistoryHeader* | Header name for storing conversation history | CamelOpenAIConversationHistory | String
|===

== Message Headers

=== Input Headers

[width="100%",cols="2,5,^1,2",options="header"]
|===
| Name | Description | Default | Type
| *CamelOpenAIUserPrompt* | User prompt text. Required when body is a File (for image files) or when you want to combine a prompt with file content (for text files). | | String
| *CamelOpenAIModel* | Override the model for this message | | String
| *CamelOpenAITemperature* | Override the temperature for this message | | Double
| *CamelOpenAITopP* | Override the top P for this message | | Double
| *CamelOpenAIMaxTokens* | Override the max tokens for this message | | Integer
| *CamelOpenAIStreaming* | Override streaming setting for this message | | Boolean
| *CamelOpenAIOutputClass* | Override output class for this message | | String
| *CamelOpenAIJsonSchema* | Override JSON schema for this message | | String
| *CamelOpenAIConversationHistory* | List of previous messages for conversation memory | | List<ChatCompletionMessageParam>
|===

=== Output Headers

[width="100%",cols="2,5,2",options="header"]
|===
| Name | Description | Type
| *CamelOpenAIResponseId* | Response ID from OpenAI | String
| *CamelOpenAIResponseModel* | Model used for the response | String
| *CamelOpenAIFinishReason* | Reason why the model stopped generating. Possible values: `stop`, `length`, `content_filter`, `tool_calls`, `function_call`, `refusal` | String
| *CamelOpenAIPromptTokens* | Number of tokens in the prompt | Long
| *CamelOpenAICompletionTokens* | Number of tokens in the completion | Long
| *CamelOpenAITotalTokens* | Total number of tokens used | Long
|===

== Authentication and Configuration

The component reads credentials and configuration from multiple sources in the following priority order:

1. URI parameters (highest priority)
2. Component-level configuration
3. Environment variables (`OPENAI_API_KEY`)
4. System properties / application.properties (`openai.api.key`)

NOTE: The API key can be omitted if using OpenAI-compatible providers that don't require authentication (e.g., some local LLM servers).

== Examples

=== Basic Chat Completion with String Input

[source,java]
----
from("direct:chat")
    .setBody(constant("What is Apache Camel?"))
    .to("openai:chat-completion?model=gpt-5")
    .log("Response: ${body}");
----

=== Using Configuration from Environment

[source,java]
----
// Assumes OPENAI_API_KEY is set in environment
from("timer:question?repeatCount=1")
    .setBody(constant("Explain microservices in one sentence"))
    .to("openai:chat-completion")
    .log("${body}");
----

=== File-Backed Prompt with Text File

[source,java]
----
from("file:prompts?noop=true")
    .to("openai:chat-completion?model=gpt-5")
    .log("Response: ${body}");
----

The component automatically reads text files (detected by MIME type starting with `text/`) and uses the file content as the prompt. Alternatively, you can provide a user prompt header to combine with the file content:

[source,java]
----
from("file:prompts?noop=true")
    .setHeader("CamelOpenAIUserPrompt", constant("Summarize this document:"))
    .to("openai:chat-completion?model=gpt-5")
    .log("Response: ${body}");
----

=== Image File Input with Vision Model

[source,java]
----
from("file:images?noop=true")
    .setHeader("CamelOpenAIUserPrompt", constant("Describe what you see in this image"))
    .to("openai:chat-completion?model=gpt-4.1-mini")
    .log("Response: ${body}");
----

NOTE: When using image files, the `CamelOpenAIUserPrompt` header is required. Supported image formats are detected by MIME type (e.g., `image/png`, `image/jpeg`, `image/gif`, `image/webp`).

=== Streaming Response

When `streaming=true`, the component returns an `Iterator<ChatCompletionChunk>` in the message body. You can consume this iterator using Camel's streaming EIPs or process it directly:

[source,yaml]
----
- route:
    id: route-1145
    from:
      id: from-1972
      uri: timer
      parameters:
        repeatCount: 1
        timerName: timer
      steps:
        - setBody:
            constant: In one sentence, what is Apache Camel?
        - to: openai:chat-completion?model=gpt-5&streaming=true
        - split:
            id: split-3196
            steps:
              - marshal:
                  id: marshal-3773
                  json:
                    library: Jackson
              - log:
                  id: log-6722
                  message: ${body}
            simple:
              expression: ${body}
            streaming: true
----

=== Structured Output with outputClass

[source,java]
----
public class Person {
    public String name;
    public int age;
    public String occupation;
}

from("direct:structured")
    .setBody(constant("Generate a person profile for a software engineer"))
    .to("openai:chat-completion?outputClass=com.example.Person&model=gpt-5")
    .log("Structured response: ${body}");
----

=== Structured Output with JSON Schema

The `jsonSchema` option instructs the model to return JSON that conforms to the provided schema. The response will be valid JSON but is not automatically validated against the schema:

[source,java]
----
from("direct:json-schema")
    .setBody(constant("Create a product description"))
    .setHeader("CamelOpenAIJsonSchema", constant("{\"type\":\"object\",\"properties\":{\"name\":{\"type\":\"string\"},\"price\":{\"type\":\"number\"}}}"))
    .to("openai:chat-completion?model=gpt-5")
    .log("JSON response: ${body}");
----

You can also load the schema from a resource file:

[source,java]
----
from("direct:json-schema-resource")
    .setBody(constant("Create a product description"))
    .setHeader("CamelOpenAIJsonSchema")
        .simple("resource:classpath:schemas/product.schema.json")
    .to("openai:chat-completion?model=gpt-5")
    .log("JSON response: ${body}");
----

NOTE: For full schema validation, integrate with the `camel-json-validator` component after receiving the response.

=== Conversation Memory (Per Exchange)

[source,java]
----
from("direct:conversation")
    .setBody(constant("My name is Alice"))
    .to("openai:chat-completion?conversationMemory=true")
    .log("First response: ${body}")
    .setBody(constant("What is my name?"))
    .to("openai:chat-completion?conversationMemory=true")
    .log("Second response: ${body}"); // Will remember "Alice"
----

=== Using Third-Party or Local OpenAI-Compatible Endpoint

[source,java]
----
from("direct:local")
    .setBody(constant("Hello from local LLM"))
    .to("openai:chat-completion?baseUrl=http://localhost:1234/v1&model=local-model")
    .log("${body}");
----

=== Dynamic Parameters via Headers

[source,java]
----
from("direct:dynamic")
    .setBody(constant("Write a haiku"))
    .setHeader("CamelOpenAIModel", constant("gpt-5"))
    .setHeader("CamelOpenAITemperature", constant(0.7))
    .setHeader("CamelOpenAIMaxTokens", constant(100))
    .to("openai:chat-completion")
    .log("${body}");
----

== Input Handling

The component accepts the following types of input in the message body:

1. *String*: The prompt text is taken directly from the body
2. *File*: Used for file-based prompts. The component handles two types of files:
   * *Text files* (MIME type starting with `text/`): The file content is read and used as the prompt. Optionally, you can set `CamelOpenAIUserPrompt` to combine a prompt with the file content
   * *Image files* (MIME type starting with `image/`): The file is encoded as a base64 data URL and sent to vision-capable models. The `CamelOpenAIUserPrompt` header is **required** when using image files

NOTE: When using `File` input, the component uses `Files.probeContentType()` to detect the file type. Ensure your system has proper MIME type detection configured.

== Output Handling

=== Default Mode
The full model response is returned as a String in the message body.

=== Streaming Mode
When `streaming=true`, the message body contains an `Iterator<ChatCompletionChunk>` suitable for Camel streaming EIPs (such as `split()` with `streaming()`). 

IMPORTANT: 
* Resource cleanup is handled automatically when the Exchange completes (success or failure)
* Conversation memory is **not** automatically updated for streaming responses (only for non-streaming responses)

=== Structured Outputs

==== Using outputClass
The response body is automatically deserialized to an instance of the specified class.

==== Using jsonSchema
The `jsonSchema` option instructs the model to return JSON conforming to the provided schema. The response will be valid JSON but is not automatically validated against the schema. For full schema validation, integrate with the `camel-json-validator` component after receiving the response.

The JSON schema must be a valid JSON object. Invalid schema strings will result in an `IllegalArgumentException`.

== Conversation Memory

When `conversationMemory=true`, the component maintains conversation history in the `CamelOpenAIConversationHistory` header (configurable via `conversationHistoryHeader` option). This history is scoped to a single Exchange and allows multi-turn conversations within a route.

IMPORTANT:
* Conversation history is automatically updated with each assistant response for **non-streaming** responses only
* The history is stored as a `List<ChatCompletionMessageParam>` in the Exchange header
* The history persists across multiple calls to the endpoint within the same Exchange
* You can manually set the `CamelOpenAIConversationHistory` header to provide custom conversation context

Example of manual conversation history:

[source,java]
----
List<ChatCompletionMessageParam> history = new ArrayList<>();
history.add(ChatCompletionMessageParam.ofUser(/* ... */));
history.add(ChatCompletionMessageParam.ofAssistant(/* ... */));

from("direct:with-history")
    .setBody(constant("Continue the conversation"))
    .setHeader("CamelOpenAIConversationHistory", constant(history))
    .to("openai:chat-completion?conversationMemory=true")
    .log("${body}");
----

== Compatibility

This component works with any OpenAI API-compatible endpoint by setting the `baseUrl` parameter. This includes:

- OpenAI official API (default: `https://api.openai.com/v1`)
- Azure OpenAI (may require additional configuration)
- Local LLM servers (e.g., Ollama, LM Studio, LocalAI)
- Third-party OpenAI-compatible providers

NOTE: When using local or third-party providers, ensure they support the chat completions API endpoint format. Some providers may have different authentication requirements or API variations.

== Error Handling

The component may throw the following exceptions:

* `IllegalArgumentException`: 
  ** When an invalid operation is specified (only `chat-completion` is supported)
  ** When message body or user prompt is missing
  ** When image file is provided without `CamelOpenAIUserPrompt` header
  ** When unsupported file type is provided (only text and image files are supported)
  ** When invalid JSON schema string is provided
* `IllegalStateException`: When the endpoint or client cannot be initialized
* API-specific exceptions from the OpenAI SDK for network errors, authentication failures, rate limiting, etc.

== Dependencies

Add the following dependency to your project:

[source,xml]
----
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-openai</artifactId>
    <version>x.x.x</version>
</dependency>
----
