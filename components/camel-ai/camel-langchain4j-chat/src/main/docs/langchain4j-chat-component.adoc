= langChain4j Chat Component
:doctitle: langChain4j Chat
:shortname: langchain4j-chat
:artifactid: camel-langchain4j-chat
:description: LangChain4j Chat component
:since: 4.5
:supportlevel: Stable
:tabs-sync-option:
:component-header: Only producer is supported
//Manually maintained attributes
:camel-spring-boot-name: langchain4j-chat

*Since Camel {since}*

*{component-header}*

The LangChain4j Chat Component allows you to integrate with any LLM supported by https://github.com/langchain4j/langchain4j[LangChain4j].

Maven users will need to add the following dependency to their `pom.xml`
for this component:

[source,xml]
----
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-langchain4j-chat</artifactId>
    <version>x.x.x</version>
    <!-- use the same version as your Camel core version -->
</dependency>
----

== URI format

[source]
----
langchain4j-chat:chatIdId[?options]
----

Where *chatId* can be any string to uniquely identify the endpoint


// component-configure options: START

// component-configure options: END

// component options: START
include::partial$component-configure-options.adoc[]
include::partial$component-endpoint-options.adoc[]
// component options: END

// endpoint options: START

// endpoint options: END

include::spring-boot:partial$starter.adoc[]

== Using a specific Chat Model
The Camel LangChain4j chat component provides an abstraction for interacting with various types of Large Language Models (LLMs) supported by https://github.com/langchain4j/langchain4j[LangChain4j].

To integrate with a specific Large Language Model, users should follow these steps:

=== Example of Integrating with OpenAI
Add the dependency for LangChain4j OpenAI support:

[source,xml]
----
<dependency>
      <groupId>dev.langchain4j</groupId>
      <artifactId>langchain4j-open-ai</artifactId>
    <version>x.x.x</version>
</dependency>
----

Init the OpenAI Chat Language Model, add add it to the Camel Registry:
[source, java]
----
ChatLanguageModel model = OpenAiChatModel.builder()
                .apiKey(openApiKey)
                .modelName(GPT_3_5_TURBO)
                .temperature(0.3)
                .timeout(ofSeconds(3000))
                .build();
context.getRegistry().bind("chatModel", model);
----

Use the model in the Camel LangChain4j Chat Producer
[source, java]
----
 from("direct:chat")
      .to("langchain4j-chat:test?chatModel=#chatModel")

----

[NOTE]
====
To switch to another Large Language Model and its corresponding dependency, simply replace the `langchain4j-open-ai` dependency with the appropriate dependency for the desired model. Update the initialization parameters accordingly in the code snippet provided above.
====

== Send a prompt with variables
To send a prompt with variables, use the Operation type `LangChain4jChatOperations.CHAT_SINGLE_MESSAGE_WITH_PROMPT`.
This operation allows you to send a single prompt message with dynamic variables, which will be replaced with values provided in the request.

Example of route :
[source, java]
----
 from("direct:chat")
      .to("langchain4j-chat:test?chatModel=#chatModel&chatOperation=CHAT_SINGLE_MESSAGE_WITH_PROMPT")

----

Example of usage:
[source, java]
----
var promptTemplate = "Create a recipe for a {{dishType}} with the following ingredients: {{ingredients}}";

Map<String, Object> variables = new HashMap<>();
variables.put("dishType", "oven dish");
variables.put("ingredients", "potato, tomato, feta, olive oil");

String response = template.requestBodyAndHeader("direct:chat", variables,
                LangChain4jChat.Headers.PROMPT_TEMPLATE, promptTemplate, String.class);
----

== Chat with history
You can send a new prompt along with the chat message history by passing all messages in a list of type `dev.langchain4j.data.message.ChatMessage`.
Use the Operation type `LangChain4jChatOperations.CHAT_MULTIPLE_MESSAGES`.
This operation allows you to continue the conversation with the context of previous messages.

Example of route :
[source, java]
----
 from("direct:chat")
      .to("langchain4j-chat:test?chatModel=#chatModel&chatOperation=CHAT_MULTIPLE_MESSAGES")

----

Example of usage:
[source, java]
----
List<ChatMessage> messages = new ArrayList<>();
messages.add(new SystemMessage("You are asked to provide recommendations for a restaurant based on user reviews."));
// Add more chat messages as needed

String response = template.requestBody("direct:send-multiple", messages, String.class);
----

== Chat with Tool
Camel langchain4j-chat used as a consumer can be used to implement a Tool,
right now Tools are supported only via the OpenAiChatModel
backed by OpenAI APIs.

Tool Input parameter can be defined via parameterName and parameterType in case of only one parameter,
or via the endpoint option camelToolParameter in case of multiple parameters.
The parameters can be found as headers in the consumer route, in particular, if you define a `parameterName=userId`,
in the consumer route `${header.userId}` can be used.

Example of a producer and a consumer:
[source, java]
----
from("direct:test")
    .to("langchain4j-chat:test1?chatOperation=CHAT_MULTIPLE_MESSAGES");

from("langchain4j-chat:test1?description=Query user database by number&parameterName=number&parameterType=integer")
    .to("sql:SELECT name FROM users WHERE id = :#number");
----

Example of usage:
[source, java]
----
List<ChatMessage> messages = new ArrayList<>();
        messages.add(new SystemMessage("""
                You provide information about specific user name querying the database given a number.
                """));
        messages.add(new UserMessage("""
                What is the name of the user 1?
                """));

        Exchange message = fluentTemplate.to("direct:test").withBody(messages).request(Exchange.class);
----