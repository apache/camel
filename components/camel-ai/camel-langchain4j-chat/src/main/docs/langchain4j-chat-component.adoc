= LangChain4j Chat Component
:doctitle: LangChain4j Chat
:shortname: langchain4j-chat
:artifactid: camel-langchain4j-chat
:description: LangChain4j Chat component
:since: 4.5
:supportlevel: Stable
:tabs-sync-option:
:component-header: Both producer and consumer are supported
//Manually maintained attributes
:group: AI
:camel-spring-boot-name: langchain4j-chat

*Since Camel {since}*

*{component-header}*

The LangChain4j Chat Component allows you to integrate with any Large Language Model (LLM) supported by https://github.com/langchain4j/langchain4j[LangChain4j].

Maven users will need to add the following dependency to their `pom.xml`
for this component:

[source,xml]
----
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-langchain4j-chat</artifactId>
    <version>x.x.x</version>
    <!-- use the same version as your Camel core version -->
</dependency>
----

== URI format

----
langchain4j-chat:chatId[?options]
----

Where *chatId* can be any string to uniquely identify the endpoint


// component options: START
include::partial$component-configure-options.adoc[]
include::partial$component-endpoint-options.adoc[]
include::partial$component-endpoint-headers.adoc[]
// component options: END

include::spring-boot:partial$starter.adoc[]

== Usage

=== Using a specific Chat Model

The Camel LangChain4j chat component provides an abstraction for interacting with various types of Large Language Models (LLMs) supported by https://github.com/langchain4j/langchain4j[LangChain4j].

==== Integrating with specific LLM

To integrate with a specific LLM, users should follow the steps described below, which explain
how to integrate with OpenAI.

===== Using LangChain4j Spring Boot Starters (Recommended for Spring Boot)

When using Camel with Spring Boot, you can leverage LangChain4j's Spring Boot starters for automatic configuration.

Add the dependency for LangChain4j OpenAI Spring Boot starter:

.pom.xml
[source,xml]
----
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j-open-ai-spring-boot-starter</artifactId>
    <version>1.10.0</version>
    <!-- use the same version as your LangChain4j version -->
</dependency>
----

Configure the OpenAI Chat Model in `application.properties` or `application.yml`:

.application.properties
[source,properties]
----
langchain4j.open-ai.chat-model.api-key=${OPENAI_API_KEY}
langchain4j.open-ai.chat-model.model-name=gpt-3.5-turbo
langchain4j.open-ai.chat-model.temperature=0.3
langchain4j.open-ai.chat-model.timeout=3000s
----

.application.yml
[source,yaml]
----
langchain4j:
  open-ai:
    chat-model:
      api-key: ${OPENAI_API_KEY}
      model-name: gpt-3.5-turbo
      temperature: 0.3
      timeout: 3000s
----

The `ChatLanguageModel` bean will be automatically configured and available in the Spring context. Use it in your Camel routes:

[tabs]
====
Java::
+
[source,java]
----
from("direct:chat")
    .to("langchain4j-chat:test?chatModel=#chatLanguageModel");
----

YAML::
+
[source,yaml]
----
- route:
    from:
      uri: "direct:chat"
    steps:
      - to:
          uri: "langchain4j-chat:test"
          parameters:
            chatModel: "#chatLanguageModel"
----

XML::
+
[source,xml]
----
<route>
  <from uri="direct:chat"/>
  <to uri="langchain4j-chat:test?chatModel=#chatLanguageModel"/>
</route>
----
====

[NOTE]
====
LangChain4j Spring Boot starters provide auto-configuration for various LLM providers including:

* `langchain4j-open-ai-spring-boot-starter` - OpenAI
* `langchain4j-azure-open-ai-spring-boot-starter` - Azure OpenAI
* `langchain4j-google-ai-gemini-spring-boot-starter` - Google Gemini
* `langchain4j-ollama-spring-boot-starter` - Ollama
* `langchain4j-anthropic-spring-boot-starter` - Anthropic Claude
* `langchain4j-mistral-ai-spring-boot-starter` - Mistral AI
* `langchain4j-hugging-face-spring-boot-starter` - Hugging Face

For a complete list of available starters and their configuration options, refer to the https://docs.langchain4j.dev/tutorials/spring-boot-integration[LangChain4j Spring Boot Integration documentation].
====

===== Manual Configuration (Alternative)

Alternatively, you can manually initialize the Chat Model and add it to the Camel Registry:

Add the dependency for LangChain4j OpenAI support:

.pom.xml
[source,xml]
----
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j-open-ai</artifactId>
    <version>1.10.0</version>
    <!-- use the same version as your LangChain4j version -->
</dependency>
----

Initialize the OpenAI Chat Model, and add it to the Camel Registry:

[source,java]
----
ChatLanguageModel model = OpenAiChatModel.builder()
    .apiKey(openApiKey)
    .modelName(GPT_3_5_TURBO)
    .temperature(0.3)
    .timeout(ofSeconds(3000))
    .build();
context.getRegistry().bind("myChatModel", model);
----

Use the model in the Camel LangChain4j Chat Producer:

[tabs]
====
Java::
+
[source,java]
----
from("direct:chat")
    .to("langchain4j-chat:test?chatModel=#myChatModel");
----

YAML::
+
[source,yaml]
----
- route:
    from:
      uri: "direct:chat"
    steps:
      - to:
          uri: "langchain4j-chat:test"
          parameters:
            chatModel: "#myChatModel"
----
====

[NOTE]
====
To switch to another Large Language Model and its corresponding dependency, replace the `langchain4j-open-ai` dependency with the appropriate dependency for the desired model. Update the initialization parameters accordingly in the code snippet provided above.
====

=== Send a prompt with variables

To send a prompt with variables, use the Operation type `LangChain4jChatOperations.CHAT_SINGLE_MESSAGE_WITH_PROMPT`.
This operation allows you to send a single prompt message with dynamic variables, which will be replaced with values provided in the request.

.Route example:
[source, java]
----
 from("direct:chat")
      .to("langchain4j-chat:test?chatModel=#chatModel&chatOperation=CHAT_SINGLE_MESSAGE_WITH_PROMPT")

----

.Usage example:
[source, java]
----
var promptTemplate = "Create a recipe for a {{dishType}} with the following ingredients: {{ingredients}}";

Map<String, Object> variables = new HashMap<>();
variables.put("dishType", "oven dish");
variables.put("ingredients", "potato, tomato, feta, olive oil");

String response = template.requestBodyAndHeader("direct:chat", variables,
                LangChain4jChat.Headers.PROMPT_TEMPLATE, promptTemplate, String.class);
----

=== Chat with history

You can send a new prompt along with the chat message history by passing all messages in a list of type `dev.langchain4j.data.message.ChatMessage`.
Use the Operation type `LangChain4jChatOperations.CHAT_MULTIPLE_MESSAGES`.
This operation allows you to continue the conversation with the context of previous messages.

.Route example:
[source, java]
----
 from("direct:chat")
      .to("langchain4j-chat:test?chatModel=#chatModel&chatOperation=CHAT_MULTIPLE_MESSAGES")

----

.Usage example:
[source, java]
----
List<ChatMessage> messages = new ArrayList<>();
messages.add(new SystemMessage("You are asked to provide recommendations for a restaurant based on user reviews."));
// Add more chat messages as needed

String response = template.requestBody("direct:send-multiple", messages, String.class);
----

=== Retrieval Augmented Generation (RAG)

Use the RAG feature to enrich exchanges with data retrieved from any type of Camel endpoint. The feature is compatible with all LangChain4 Chat operations and is ideal for orchestrating the RAG workflow, utilizing the extensive library of components and Enterprise Integration Patterns (EIPs) available in Apache Camel.

There are two ways for utilizing the RAG feature:

==== Using RAG with Content Enricher and LangChain4jRagAggregatorStrategy
Enrich the exchange by retrieving a list of strings using any Camel producer. The `LangChain4jRagAggregatorStrategy` is specifically designed to augment data within LangChain4j chat producers.

.Usage example:
[source, java]
----
// Create an instance of the RAG aggregator strategy
LangChain4jRagAggregatorStrategy aggregatorStrategy = new LangChain4jRagAggregatorStrategy();

from("direct:test")
     .enrich("direct:rag", aggregatorStrategy)
     .to("langchain4j-chat:test1?chatOperation=CHAT_SIMPLE_MESSAGE");

  from("direct:rag")
          .process(exchange -> {
                List<String> augmentedData = List.of("data 1", "data 2" );
                exchange.getIn().setBody(augmentedData);
              });
----

[NOTE]
====
This method leverages a separate Camel route to fetch and process the augmented data.
====

It is possible to enrich the message from multiple sources within the same exchange.

.Usage example:
[source, java]
----
// Create an instance of the RAG aggregator strategy
LangChain4jRagAggregatorStrategy aggregatorStrategy = new LangChain4jRagAggregatorStrategy();

from("direct:test")
     .enrich("direct:rag-from-source-1", aggregatorStrategy)
     .enrich("direct:rag-from-source-2", aggregatorStrategy)
     .to("langchain4j-chat:test1?chatOperation=CHAT_SIMPLE_MESSAGE");
----

==== Using RAG with headers

Directly add augmented data into the header. This method is particularly efficient for straightforward use cases where the augmented data is predefined or static.
You must add augmented data as a List of `dev.langchain4j.rag.content.Content` directly inside the header `CamelLangChain4jChatAugmentedData`.

.Usage example:
[source, java]
----
import dev.langchain4j.rag.content.Content;

...

Content augmentedContent = new Content("data test");
List<Content> contents = List.of(augmentedContent);

String response = template.requestBodyAndHeader("direct:send-multiple", messages, LangChain4jChat.Headers.AUGMENTED_DATA , contents, String.class);
----
